{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Tutorial and Revision Manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.1 A Bird's Eye View on MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series of returns\n",
    "\n",
    "$$\n",
    "r_1, r_2, ..., r_T.\n",
    "$$\n",
    "\n",
    "has a joint probability density function  \n",
    "\n",
    "$$\n",
    "f(r_{T},\\ r_{T-1},\\ \\cdots,\\ r_{2},\\ r_{1};\\theta).\n",
    "$$\n",
    "\n",
    "which can be written as\n",
    "\n",
    "$$\n",
    "f(r_{T},\\ r_{T-1},\\ \\cdots,\\ r_{2},\\ r_{1};\\theta)\n",
    "=\\left[ \\prod_{t=2}^{T}f(r_{t}|r_{t-1},\\ \\cdots,\\ r_{1};\\theta) \\right] \\times f(r_{1};\\theta).  \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "First, if returns are independent of each other:\n",
    "\n",
    "$$\n",
    "f(r_{T},\\ r_{T-1},\\ \\cdots,\\ r_{2},\\ r_{1};\\theta) \\overbrace{=}^{indep} \\,  \\left[ \\prod_{t=1}^{T}f(r_{t};\\theta) \\right].\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Second, if returns are Markov: \n",
    "\n",
    "$$\n",
    "f(r_{T},\\ r_{T-1},\\ \\cdots,\\ r_{2},\\ r_{1};\\theta) \\overbrace{=}^{Markov} \\left[ \\prod_{t=2}^{T}f(r_{t}|r_{t-1};\\theta) \\right]\\times f(r_{1};\\theta).\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "In order to highlight that this joint likelihood function depends on $\\theta$, we often write it as \n",
    "\n",
    "$$\n",
    "L(\\theta|\\{r_t\\}_{t \\in [1,...,T]}) := f(r_{T},\\ r_{T-1},\\ \\cdots,\\ r_{2},\\ r_{1};\\theta), \n",
    "$$ \n",
    "\n",
    "or in short, simply $L(\\theta)$. \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "**Three optimization approaches:** \n",
    "\n",
    "- Analytic Optimization  \n",
    "\n",
    "\n",
    "- Grid Search\n",
    "\n",
    "\n",
    "- Numerical Optimization  \n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Statistical properties of MLE in LARGE samples:** \n",
    "\n",
    "- Consistent; plim $\\hat{\\theta}_{ML}=\\theta$  \n",
    "\n",
    "- Asymptotically Normally Distributed; $\\hat{\\theta}_{ML}\\sim N[\\theta,\\ \\{I(\\theta)\\}^{-1}]$,  \n",
    "\n",
    "- Variance of the MLE fulfills the Rao-Cramer lower bound,i.e. 'most efficient' (most precise) estimator  \n",
    "\n",
    "- Invariance: rather than estimating a parameter $\\theta$, we can instead estimate $g(\\theta)$ and recover the MLE of $\\theta$ from inverting $g(\\theta)$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Statistical properties of MLE in SMALL samples:** \n",
    "\n",
    "- not well understood\n",
    "\n",
    "- do not use less than 100 observations \n",
    " \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.2 What's the Probability of a 10% Decline in the S&P 500? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of observing in a year five days with a negative return of more than 10\\% would be\n",
    "\n",
    "$$\n",
    "Prob(y_t=5;\\theta) \\overbrace{=}^{Pois} \\, \\frac{e^{-\\lambda} \\lambda^{5}}{5!}.\n",
    "$$\n",
    "\n",
    "The joint likelihood equals \n",
    "\n",
    "\\begin{align*}\n",
    "L(\\lambda) & = \\frac{e^{-\\lambda}\\lambda^{y_{1}}}{y_{1}!}\\times\\frac{e^{-\\lambda}\\lambda^{y_{2}}}{y_{2}!}\\times\\ldots\\times\\frac{e^{-\\lambda}\\lambda^{y_{T}}}{y_{T}!} \\\\\n",
    "&= \\prod_{t=1}^{T}\\frac{e^{-\\lambda}\\lambda^{y_{t}}}{y_{t}!}.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The $\\log$ likelihood is\n",
    "\n",
    "$$\n",
    "\\ln L(\\lambda) \\ =\\  -T\\lambda\\ +\\sum_{t=1}^T y_{t}\\ln(\\lambda)-\\sum_{t=1}^T \\ln(y_{t}!)\n",
    "$$\n",
    "\n",
    "The MLE of $\\lambda$ maximizes $\\ln L(\\lambda)$; hence leads to a zero gradient, i.e.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ln L(\\lambda)}{\\partial\\lambda}=-T+\\frac{\\sum_{t=1}^{T}y_{t}}{\\lambda} \\overbrace{=}^{!} 0\n",
    "$$\n",
    "\n",
    "We solve for $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\hat{\\lambda}_{ML}\\ =\\ \\frac{\\sum_{t=1}^{T}y_{t}}{T}.\n",
    "$$\n",
    "\n",
    "and check that the second derivative is negative.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.3 How Precise are MLE Parameter Estimates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian matrix, i.e. $\\frac{\\partial^2 \\ln L(\\theta)}{\\partial \\theta \\, \\partial \\theta'}$,  captures the 2nd derivative of the log-likelihood function and therefore the curvature of the log likelihood function. Hence, the more curvature, the larger the Hessian and the smaller the inverse of the Hessian and the smaller the standard error of $\\hat{\\theta}_{ML}$. \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "The MLE standard error coincides with the square root of the variance of the MLE estimator $\\hat{\\theta}_{ML}$. \n",
    "\n",
    "$$\n",
    "var\\ (\\hat{\\theta}_{ML})=[I(\\hat{\\theta}_{ML})]^{-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "For a maximization problem, the ith-jth entry of the information matrix equals:\n",
    "\n",
    "\\begin{align*}\n",
    "[I(\\theta)]_{[i,j]} &:= E \\left[ \\left( \\frac{\\partial \\ln L(\\theta)}{\\partial \\theta_i} \\right) \\left( \\frac{\\partial \\ln L(\\theta)}{\\partial \\theta_j} \\right) \\right]   \\\\\n",
    "&= -E \\left[ \\frac{\\partial^{2}\\ln {L}(\\theta)}{\\partial\\theta_i\\partial\\theta_j} \\right].\n",
    "\\end{align*}\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "Notice, for a minimization problem you multiply the above expression by $-1$!\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "The standard error of the vector-valued estimator, $\\hat{\\theta}_{ML}$, coincides with the square root of the diagonal elements of $var (\\hat{\\theta}_{ML})$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "'Observed Fisher Information Matrix', which for a maximization problem reads\n",
    "\n",
    "\\begin{align*}\n",
    "[I(\\theta)]_{[i,j]} &:=  \\left( \\frac{\\partial \\ln L(\\theta)}{\\partial \\theta_i} \\right) \\left( \\frac{\\partial \\ln L(\\theta)}{\\partial \\theta_j} \\right) \\\\\n",
    "&= -\\frac{\\partial^{2}\\ln {L}(\\theta)}{\\partial\\theta_i\\partial\\theta_j}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\ln L = \\sum_t \\ln L_t$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.4 Computing Hessian Matrices using 1st or 2nd Derivatives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a maximization problem, you compute the Fisher Information matrix with either of both choices\n",
    "\n",
    "$$\n",
    "I(\\theta) \\equiv E \\left[ \\left( \\frac{\\partial \\ln L(X;\\theta)}{\\partial \\theta} \\right)^2 \\right] = - E \\left[  \\frac{\\partial^2 \\ln L(X;\\theta)}{\\partial \\theta^2} \\right].\n",
    "$$\n",
    "\n",
    "\n",
    "Let's define $f:=L(X;\\theta)$ and note that $f = f(\\theta)$.\n",
    "\n",
    "\\begin{align*}\n",
    "- E \\left[  \\frac{\\partial^2 \\ln L(X;\\theta)}{\\partial \\theta^2} \\right] &= - E \\left[  \\frac{\\partial}{\\partial \\theta} \\left( 1/L(X;\\theta) \\times \\frac{\\partial L(X;\\theta)}{\\partial \\theta} \\right) \\right] \\\\\n",
    "&= E\\left[ \\frac{1}{f^2} f' f' - \\frac{1}{f} f'' \\right] \\\\\n",
    "&= E\\left[ \\left( \\frac{f'}{f} \\right)^2 \\right] -  E\\left[ \\frac{f''}{f} \\right] \\\\\n",
    "&= E [ ((\\ln f)')^2] - E[\\frac{f''}{f}] \\\\\n",
    "&= E [ ((\\ln f)')^2 \\equiv I(\\theta).\n",
    "\\end{align*}\n",
    "\n",
    "where we used the product rule: \n",
    "\n",
    "$$\n",
    "D(fg) = fg' + f'g. \n",
    "$$\n",
    "\n",
    "The term -$E[\\frac{f''}{f}] = 0$\n",
    "\n",
    "because\n",
    "\n",
    "\\begin{align*}\n",
    "-E[\\frac{f''}{f}] &= - \\frac{\\partial^2}{\\partial \\theta^2} \\int_{-\\infty}^{\\infty} f(x;\\theta) \\times \\frac{f(x;\\theta)}{f(x;\\theta)} dx \\\\\n",
    "&= - \\frac{\\partial^2}{\\partial \\theta^2} 1 = 0.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.5 Learning about MLE using a Linear Problem, i.e. G-LRM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the linear regression problem with Gaussian regression error:\n",
    "\n",
    "$$\n",
    "y = X \\beta + \\epsilon; \\; \\epsilon \\sim N(0,\\sigma^2 I)\n",
    "$$ \n",
    "with $dim(y) = T\\times 1$ and $dim(X) = T\\times k$ and $dim(\\beta) = k \\times 1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.5.1 Deriving the Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood for a single observation $y_i$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y_{i}|x_i,\\ \\beta,\\ \\sigma^{2})=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(y_{i}-x_{i}\\beta)^{2}}{2\\sigma^{2}}}\n",
    "$$\n",
    "\n",
    "where $dim(x_i) = 1 \\times k$.\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "If $\\epsilon$ is iid, then\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y|X,\\ \\beta,\\ \\sigma^{2})=\\prod_{i=1}^{T}\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(y_{i}-x_{i}\\beta)^{2}}{2\\sigma^{2}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "which in log and matrix notation reads:\n",
    "\n",
    "\n",
    "$$\n",
    "\\ln \\mathcal{L}=\\frac{-T}{2}\\ln(2\\pi)-\\frac{T}{2}\\ln(\\sigma^{2})-\\frac{1}{2\\sigma^{2}}(y-X\\beta)'(y-X\\beta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "- appreciate the similarity to a L2 objective function\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "- appreciate how the linear G-LRM objective function separates parameters from variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.5.2 Optimizing $ln (L)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint log likelihood function equals:\n",
    "\n",
    "$$\n",
    "\\ln \\mathcal{L}(y|X,\\ \\beta,\\ \\sigma^{2})\\ =\\ -\\frac{T}{2}\\ln(2\\pi)-\\frac{T}{2}\\ln(\\sigma^{2})-\\frac{1}{2\\sigma^{2}}[y'y-2y'X\\beta+\\beta'X'X\\beta]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**FOC: beta:**\n",
    " \n",
    "$$\n",
    "\\frac{\\partial\\ln \\mathcal{L}}{\\partial\\beta}\\ =\\ -\\frac{1}{2\\sigma^{2}}\\left[ \\frac{\\partial[y'y-2y'X\\beta+\\beta'X'X\\beta]}{\\partial\\beta} \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= ... \n",
    "$$\n",
    "\n",
    " \n",
    "$$ \n",
    "\\frac{1}{\\sigma^{2}}[X'y-X'X\\beta] \\overbrace{=}^{!} 0\n",
    "$$\n",
    "\n",
    " \n",
    "$$\n",
    "\\rightarrow \\; \\hat{\\beta} = (X'X)^{-1}X'y\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    " \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**FOC: $\\sigma^2$:**\n",
    "\n",
    " \n",
    "$$\n",
    "\\frac{\\partial\\ln \\mathcal{L}}{\\partial\\sigma^{2}}\\ =\\ -\\frac{T}{2\\sigma^{2}}+\\frac{1}{2\\sigma^{4}} \\left[(y-X\\beta)'(y-X\\beta)\\right] \\overbrace{=}^{!} 0\n",
    "$$\n",
    "\n",
    " \n",
    "$$\n",
    "...\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^{2}} \\left[(y-X\\beta)'(y-X\\beta) \\right]\\ =\\ T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Since we have already solved for $\\hat{\\beta}$, we can solve for $\\sigma^{2}$ by replacing $\\beta$ with its estimate:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^{2}}\\left[(y-X\\hat{\\beta})'(y-X\\hat{\\beta})\\right]\\ =\\ T\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\rightarrow \\; \\hat{\\sigma}^{2}=\\frac{e'e}{T}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "- appreciate: linear G-LRM allows to optimize sequentially: first for parameters, then for variance of errors. \n",
    "\n",
    "- appreciate: Fifth, even if the model was non-linear in $\\beta$ but with a Gaussian White Noise error, the ML solution for $\\sigma^2$ will coincide with $\\frac{e'e}{T}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.5.3 Getting Standard Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a maximization problem we have:\n",
    "\n",
    "$$\n",
    "\\hat{var}(\\theta) = \\left[I(\\theta)\\right]^{-1}\\ =\\ \\left(-E \\left[H(\\theta)\\right]\\right)^{-1}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Jacobian:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ln \\mathcal{L}}{\\partial\\theta}=\\begin{pmatrix} \\frac{X'(y-X\\beta)}{\\sigma^{2}}\\\\\n",
    "-\\frac{T}{2\\sigma^{2}} +\\frac{(y-X\\beta)'(y-X\\beta)}{2\\sigma^{4}}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "**Entries of Hessian:**\n",
    "\n",
    " \n",
    "$$\n",
    "\\frac{\\partial^{2}\\ln \\mathcal{L}}{\\partial\\beta \\; \\partial\\beta}= ... = -\\frac{X'X}{\\sigma^{2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\displaystyle \\frac{\\partial^{2}\\ln \\mathcal{L}}{\\partial\\beta \\; \\partial\\sigma^{2}}=...=-\\frac{X'\\epsilon}{\\sigma^{4}}\n",
    "$$\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "$$\n",
    "\\frac{\\partial^{2}\\ln \\mathcal{L}}{\\partial\\sigma^{2} \\; \\partial\\beta}\\ =...=-\\frac{\\epsilon'X}{\\sigma^{4}}\n",
    "$$\n",
    "\n",
    "\n",
    " \n",
    "$$\n",
    "\\frac{\\partial^{2}\\ln \\mathcal{L}}{\\partial\\sigma^{2} \\; \\partial\\sigma^{2}}\\ =\\ ... \\frac{T}{2\\sigma^{4}}-\\frac{\\epsilon'\\epsilon}{\\sigma^{6}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "Ergo, **Hessian matrix G-LRM:**\n",
    "\n",
    "$$\n",
    "H=\\frac{\\partial^2\\ln \\mathcal{L}}{\\partial\\theta^2 }= \\begin{pmatrix} -\\frac{X'X}{\\sigma^{2}} & -\\frac{X'\\epsilon}{\\sigma^{4}}\\\\\n",
    "-\\frac{\\epsilon'X}{\\sigma^{4}} & \\frac{T}{2\\sigma^{4}}-\\frac{\\epsilon'\\epsilon}{\\sigma^{6}}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Expectation Hessian:**\n",
    "\n",
    "$$\n",
    " E[H]=E \\begin{pmatrix} -\\frac{X'X}{\\sigma^{2}} & -\\frac{X'\\epsilon}{\\sigma^{4}}\\\\\n",
    "-\\frac{\\epsilon'X}{\\sigma^{4}} & \\frac{T}{2\\sigma^{4}}-\\frac{\\epsilon'\\epsilon}{\\sigma^{6}}\n",
    "\\end{pmatrix} = \\begin{pmatrix} -\\frac{X'X}{\\sigma^{2}} & 0 \\\\ \n",
    "0 & - \\frac{T}{2\\sigma^{4}} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    " \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Inverting Negative Expectation of Hessian:**\n",
    "\n",
    "$$\n",
    "I[\\theta]=-E[H(\\theta)]=-\\begin{pmatrix} -\\frac{X'X}{\\sigma^{2}}  & 0\\\\\n",
    "0 & - \\frac{T}{2\\sigma^{4}} \\end{pmatrix} = \\begin{pmatrix} \\frac{X'X}{\\sigma^{2}} & 0\\\\\n",
    "0 & \\frac{T}{2\\sigma^{4}} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Finally,  \n",
    "\n",
    "$$\n",
    "\\hat{var}(\\theta) \\equiv I[\\theta]^{-1}=\\begin{pmatrix} \\sigma^{2}(X'X)^{-1} & 0\\\\\n",
    "0 & \\frac{2\\sigma^{4}}{T} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Quizzes - Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**see ilias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Exercises - Basics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D.1 Decomposing Joint Probabilities\n",
    "\n",
    "Decompose the joint probability density function into marginal and conditional densities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D.2 Decomposing Joint Probabilities of Markov Processes\n",
    "\n",
    "Decompose the joint probability density function of a markov process into marginal and conditional densities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D.3 Asymptotics of MLE\n",
    "\n",
    "State the asymptotic distribution of any Maximum Likelihood Estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D.4 G-LRM\n",
    "\n",
    "Write down the linear regression model with Gaussian errors and define each term of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D.5 Likelihood of G-LRM\n",
    "\n",
    "Assume you oberve 100 observations of $\\{y_i,x_i\\}$ with $i=1,…,100$, drawn from a Gaussian linear regression model. Write down the joint likelihood function for the observation. Highlight the dependence on the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D.6 Least Squares vs ML Objective Function\n",
    "\n",
    "State the log likelihood function of a Gaussian linear regression model and show ist similarity and difference to a least squares objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D.7 S.E. of MLE\n",
    "\n",
    "How to compute the Maximum Likelihood standard error of the variane parameter of a linear Gaussian regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. Exercises - Challenging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E.1 MLE Non-Linear vs Linear Model\n",
    "\n",
    "Write down a linear and a non-linear regression model. Assume Gaussian errors. Write down the joint log likelihood function. Talk about similarities and dissimilarities of both likelihood. Derive analytically the estimates for parameters and the volatility of the error term. Can one estimate the unknown quantities iteratively or only jointly? What are advantages of an iterative approach? Which model structure allows an iterative approach and which structure prevents it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E.2 Start Values\n",
    "\n",
    "Choose a non-trivial problem of your choice to explain how to find smart (good) starting values for a MLE optimization.\n",
    "\n",
    "Hint: OLS is one method of choice, others exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F. Python\n",
    "\n",
    "**MU: I will upload the python code asap.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F. Python for Financial Data Analysis\n",
    "\n",
    "Dear students,\n",
    "\n",
    "We approach the time-series modeling part as follows. First, you learn about ARMA models. Second, you learn how to siumulate these. Third, you learn how to estimate these. These estimations will lead us into the machine learning aspects of our course.\n",
    "\n",
    "We are currently learning about how to estimate the parameters and residuals of a linear model. Our application of choice are ARMA models. \n",
    "\n",
    "I provide you with a set of Python applications for estimating linear regression problems using MAXIMUM LIKELIHOOD. The applications carry easily over to NON-LINEAR PROBLEMS. \n",
    "\n",
    "To keep set-up costs low I use the application from last chapter, namely, fitting an AR(3) to the 3-month government bond yield.\n",
    "\n",
    "In order to really learn, we will not use packages but rather code everything from scratch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.1 Preparing y and X for Fitting an AR(3)\n",
    "\n",
    "Here, we are fitting a linear Gaussian model. That takes the form\n",
    "$$\n",
    "y = X \\beta + \\epsilon, \\; \\epsilon \\sim N(0,\\sigma^2)\n",
    "$$\n",
    "with $y$ and $X$ being observed.\n",
    "\n",
    "Our application is to fit the level of the 3-month government bond yield (monthly data) onto its past 3 lags. Hence,\n",
    "\n",
    "$$\n",
    "y_t = \\text{3-month gov bond yield at time t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_t = \\text{constant} | y_{t-1}    |  y_{t-2} | y_{t-3}.\n",
    "$$\n",
    "\n",
    "I minimize repetition by loading in the respective $y$ and $X$ from our last assignment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.read_csv(\"y_AR3_3mGovYield.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check what the y_df looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1954-07-01</th>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954-08-01</th>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954-09-01</th>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954-10-01</th>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954-11-01</th>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-01</th>\n",
       "      <td>3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-01</th>\n",
       "      <td>4.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-02-01</th>\n",
       "      <td>4.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-03-01</th>\n",
       "      <td>4.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-04-01</th>\n",
       "      <td>4.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>622 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               3\n",
       "Date            \n",
       "1954-07-01  0.72\n",
       "1954-08-01  0.92\n",
       "1954-09-01  1.01\n",
       "1954-10-01  0.98\n",
       "1954-11-01  0.93\n",
       "...          ...\n",
       "2005-12-01  3.89\n",
       "2006-01-01  4.24\n",
       "2006-02-01  4.43\n",
       "2006-03-01  4.51\n",
       "2006-04-01  4.60\n",
       "\n",
       "[622 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8885760dd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.read_csv(\"X_AR3_3mGovYield.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.index = y_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>rLag1</th>\n",
       "      <th>rLag2</th>\n",
       "      <th>rLag3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1954-07-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954-08-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954-09-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954-10-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954-11-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.88</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-01-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.88</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-02-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-03-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.43</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-04-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.43</td>\n",
       "      <td>4.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>622 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0  rLag1  rLag2  rLag3\n",
       "Date                                \n",
       "1954-07-01  1.0   0.64   0.76   0.97\n",
       "1954-08-01  1.0   0.72   0.64   0.76\n",
       "1954-09-01  1.0   0.92   0.72   0.64\n",
       "1954-10-01  1.0   1.01   0.92   0.72\n",
       "1954-11-01  1.0   0.98   1.01   0.92\n",
       "...         ...    ...    ...    ...\n",
       "2005-12-01  1.0   3.88   3.71   3.42\n",
       "2006-01-01  1.0   3.89   3.88   3.71\n",
       "2006-02-01  1.0   4.24   3.89   3.88\n",
       "2006-03-01  1.0   4.43   4.24   3.89\n",
       "2006-04-01  1.0   4.51   4.43   4.24\n",
       "\n",
       "[622 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.2 MLE from Scratch: Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We summarize the key equations\n",
    "\n",
    "1. A linear Gaussian LRM ($y=X \\beta + \\epsilon$ with $\\epsilon \\sim N(0,\\sigma^2)$) has join log likelihood function of\n",
    "\n",
    "\n",
    "$$\n",
    "\\ln \\mathcal{L}=\\frac{-T}{2}\\ln(2\\pi)-\\frac{T}{2}\\ln(\\sigma^{2})-\\frac{1}{2\\sigma^{2}}(y-X\\beta)'(y-X\\beta)\n",
    "$$\n",
    "\n",
    "2. The Gaussian LRM has the following ANALYTICAL MLE solution of \n",
    "$$\n",
    "\\beta_{MLE} = \\beta_{ols} \\qquad \\text{and} \\qquad \\sigma^2_{MLE} = \\frac{1}{T} (y-X\\beta_{MLE})'(y-X\\beta_{MLE}).\n",
    "$$\n",
    "\n",
    "Hence, no numerical optimization required.\n",
    "\n",
    "3. Yet, for training purpose, we NUMERICALLY optimize the log likelihood function. This allows us to learn and to compare the numerical solution to the analytical solution. For this simple problem (linear and Gauss), we wont find a meaningful difference.\n",
    "\n",
    "4. A general numerical optimization optimizes for $\\beta$ and $\\sigma$. Yet, the computationally more efficient and robust way is to use an insight of the first order conditions (FOC) of the problem. Namely, the MLE for $\\beta$ does not depend on the MLE for $\\sigma^2$, while the latter depends on the former. \n",
    "\n",
    "5. For now, I optimize over $\\beta$ and $\\sigma^2$ jointly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.3 Joint Neg Log-Likelihood: Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get $T$ and $dim(\\beta) =:K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "622"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = y_df.shape[0]\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = X_df.shape[1]\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit-wise, let me say the following: Units should not matter as long as you keep track on what the units are. But, numerically speaking, you do not want the quantities to become too small. Rounding errors and numerical inaccuracies are not your friend and especially not when the units get too small.\n",
    "\n",
    "On the other hand, most intuition is build for annualized finance data. \n",
    "\n",
    "Last not least, the upcoming optimization did not work for me when I used monthly quantities in decimals.\n",
    "\n",
    "Hence, here, you see what works. So I am going to use $y$ and $X$ in annualized percentage terms. Hence, 9 stands for 9\\%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    5.219389\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculation wise, I prefer to have the data as numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.219389067524116"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.matrix(y_df)\n",
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.000000\n",
       "rLag1    5.213023\n",
       "rLag2    5.206994\n",
       "rLag3    5.201431\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.        , 5.21302251, 5.20699357, 5.20143087]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.matrix(X_df)\n",
    "X[:,1:] = X[:,1:]\n",
    "np.mean(X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A successfull numerical optimization starts with smart (informative) starting values. \n",
    "\n",
    "1. I know that the AR(3) coefficients are usually between -1 and 2. I want uninformative, yet realistic starting values. I hence choose \"0\".\n",
    "\n",
    "2. I know that the upper bound for $\\sigma^2$ is the variance of $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.875277761809741"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.std()**2 #a value of 7.87 stands for (7.87)%^2 meaing 7.87/10000 (decimal units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hence, I set the starting value for $\\sigma^2$ to half of the variance of $y$. That is my rather uninformative, yet not crazy guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 3.93763888])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start values for optimization\n",
    "beta0 = np.zeros(K+1) #K times beta and 1 times sigma2\n",
    "beta0[K] = 0.5 * y.std()**2\n",
    "\n",
    "beta0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we write the NEGATIVE joint log likelihood function as a python function. The input must be the parameters $\\beta$ and $\\sigma$, the output is\n",
    "\n",
    "$$\n",
    "-1 \\times ln(L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define -1* log likelihood function as a function of model parameter\n",
    "\n",
    "def _lnL(param):\n",
    "    #give input to model parameters\n",
    "    beta = param[0:K].reshape(K,1) # make it a column vector\n",
    "    sigma2 = param[K] #last entry of param MUST be variance of residual\n",
    "    \n",
    "    # evaluate model (G-LRM) at param. Record only fitting error\n",
    "    eps = y - X * beta\n",
    "    \n",
    "    #compute the G-LRM log likelihood function\n",
    "    # ln L = -T/2 * ln(2*pi) - T/2 ln(sigma2) - 1/(2*sigma2) * (y - X*beta)'(y - X*beta)\n",
    "    # Note: constant summands do not affect the soln. Hence, I could leave out the first term on the rhs. yet, I keep it in so students can match code to formula more easily\n",
    "    term1 = -(T/2) * np.log(2*np.pi) #contains no information on beta nor sigma2. could be deleted\n",
    "    term2 = -(T/2) * np.log(sigma2) #robust code ensures that sigma2 cannot be negative (prior to executing the log)\n",
    "    term3 = -1/2 * (eps.T*eps)[0,0]/sigma2\n",
    "\n",
    "    lnL_T = term1 + term2 + term3\n",
    "    \n",
    "    return (-1.0 * lnL_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test whether the function runs at least thru for the starting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3771.4419976826057\n"
     ]
    }
   ],
   "source": [
    "print(_lnL(beta0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.4 Setting up Numerical Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are setting bounds for all model parameters. We know the variance is non-negative. You don't want bounds to be too tight. make them large enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.00000000e+02, -1.00000000e+02, -1.00000000e+02, -1.00000000e+02,\n",
       "        7.87527776e-04])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lower bounds for optimization\n",
    "lb = -100 * np.ones(K+1) #initialization\n",
    "lb[K] = 0.0001*y.std()**2 #implies an R2 of (1-0.0001)\n",
    "\n",
    "lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 100., 100., 100., 100.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upper bounds for optimization\n",
    "ub = 100 * np.ones(K+1) #initialization\n",
    "\n",
    "ub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's scipy.optimize package requires to translate lb and ub into bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-100.0, 100.0),\n",
       " (-100.0, 100.0),\n",
       " (-100.0, 100.0),\n",
       " (-100.0, 100.0),\n",
       " (0.0007875277761809742, 100.0))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds = tuple( (lb[x], ub[x]) for x in range(0,len(beta0)) )\n",
    "\n",
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.5 Calling the Numerical Minimizer and handing over the Neg Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as sco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLE_AR3 = sco.minimize(fun = _lnL, x0 = beta0, bounds=bounds, method='L-BFGS-B', options={'disp':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.6 Getting the MLE Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08678835,  1.39376315, -0.61196636,  0.20252034,  0.1737547 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLE_AR3.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AR(3) parameters coincide with the OLS counterpart. \n",
    "\n",
    "The MLE estimate for $\\sigma^2$ is 0.1737547. Watch the units! As y and X were in percent, that numbers is in percent square. It might be more intuitive to state the MLE estimate for the vol\n",
    "\n",
    "$$\n",
    "std(\\epsilon)_{MLE} = \\sqrt(\\sigma^2) = \\sqrt{0.1737547} = 0.4168,\n",
    "$$\n",
    "\n",
    "which unit wise says the annualized monthly vol of interest rate shocks was 0.4168 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F.7 Numerically vs Analytically Optimal Variance of Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that for G-LRM, the optimal MLE for $\\sigma^2$ is the average value of the squared residuals. Hence, lets compute that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08678835],\n",
       "       [ 1.39376315],\n",
       "       [-0.61196636],\n",
       "       [ 0.20252034]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_MLE = MLE_AR3.x[0:K].reshape(K,1)\n",
    "\n",
    "beta_MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(622, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma2_MLE_analytical = (y.reshape(T,1)-X*beta_MLE).T * (y.reshape(T,1) - X*beta_MLE) / T\n",
    "\n",
    "sigma2_MLE_analytical.shape\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.416842367976678"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_MLE_analytical = np.sqrt(sigma2_MLE_analytical[0,0])\n",
    "\n",
    "sigma_MLE_analytical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RELATIVE numerical error for sigma_MLE is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.228506834103796e-06"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.sqrt(MLE_AR3.x[K]) - sigma_MLE_analytical) / sigma_MLE_analytical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is negligebly small. Yet, if you find the residual variance analytically, you reduce the parameter space by 20\\%. So, if you do not compute it analytically, your computer resources spend 20\\% of their time on a redundant task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}