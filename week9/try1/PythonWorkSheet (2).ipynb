{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Worksheet: Maximum Likelihood for a Nonlinear Problem\n",
    "\n",
    "Dear students,\n",
    "\n",
    "I am sorry for handing out the worksheet so late. My team and I tried hard to get a useful exercise for MLE. MLE is a crucial tool for all data science. In addition, our standard problem set was handed out as part of the python lecture notes. We hence had to come up with something useful, yet, also doable at the BSc level.\n",
    "\n",
    "The following exercise is the result. We will likely spend some time discussing your and my work during the upcoming prof cafe.\n",
    "\n",
    "\n",
    "If a significant number of learning groups communicate back that the task is too hard to accomplish till monday, I am willing to extend the final submission by one week. Yet, there will be (a then shorter) PS on vol modeling ARCH due in the same week.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember this Crucial FACT\n",
    "\n",
    "**Do NOT numerically optimize a Gaussian likelihood for a LINEAR model. Instead, take the ML parameter estimates from an OLS fit AND the ML estimate for the volatility from the respective residual**\n",
    "\n",
    "\\begin{equation}\n",
    "\\sqrt{\\frac{1}{T} \\epsilon_{ols}' \\epsilon_{ols}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  A Useful Exercise had to be Non-Linear\n",
    "\n",
    "Reason: see above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.0 Set-up of the Python Challenge\n",
    "\n",
    "Have you ever wondered how central banks, banks, hedge funds and investors know whether a government bond is fairly priced, overpriced or undervalued? Well, they use the approach of \"net present value\" to figure out what the price of a bond should be. To do so, one has to get accurate forecasts of how the short rate and risk premia move into the future. A popular approach relies on so called \"short rate models\". These models fit the time series of the short rate to a realistic, yet convenient, parametrization. This allows to obtain a forecast of where short rates will be in the future. You need to do the same for the risk premium. You might wonder why do government bonds pay a risk premium. Well, the longer the maturity of the bond, the higher the interest rate sensitivity and hence, the higher (usually) the (maturity) premium. That premium compensates for the fact that future realized interest rates will be different from todays expected future rates. Based on the CAPM, you could say that interest rate risk is systematic and hence pays a risk premium. That is for the background.\n",
    "\n",
    "A very popular short rate model is called \"Vasicek model\". That is because a mathematician, Dr. Vasicek, derived in the 1980s the term structure of interest rates under the assumption that the short rate follows a Gaussian distribution. It is fair to says that the assumed short rate follows a G-LRM. Based on stochastic calculus and the principle of net present value and no arbitrage, Dr. Vasicek derived the price of risk-free (non defaultable) bonds for different maturities. \n",
    "\n",
    "The Gaussian PDF of the short rate is\n",
    "\n",
    "\\begin{equation}\n",
    "r_t | F_{t-1} \\sim \\mathcal N\\left(\\theta^P (1- e^{-\\kappa^P})+e^{-\\kappa^P} r_{t-1}, \\sigma^2 * \\frac{1-e^{-2\\kappa^P}}{2\\kappa^P}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "Note, $\\theta^P$ is the long-run mean of $r$, $kappa^P$ is the speed of mean reversion and $\\sigma$ is the instantaneous standard deviation of the interest rate shock. As the model of Vasicek is written in continuous time, the above PDF takes several exponentials and ratios, which are the result of an integration that is NOT subject to our BSc course.\n",
    "\n",
    "Also, Dr. Vasicek derived that the arbitrage-free price of a bond at time $t$ with maturity in $\\tau$ periods, $P_t(\\tau)$ coincides with\n",
    "\n",
    "\\begin{equation}\n",
    "P_t(\\tau) = e^{A(\\tau) - B(\\tau) \\times r_t}\n",
    "\\end{equation}\n",
    "\n",
    "with\n",
    "\\begin{align*}\n",
    "B(\\tau) &:= \\frac{1-e^{-\\kappa^Q \\tau}}{\\kappa^Q} \\\\\n",
    "A(\\tau) &:= (\\theta^Q - \\frac{\\sigma^2}{2 (\\kappa^Q)^2}) \\times (  B(\\tau) - \\tau ) - \\frac{\\sigma^2}{4\\kappa^Q} B^2(\\tau) \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Note, from the CAPM and NPV you know that prices of financial assets are discounted by the risk-free rate plus risk premium. The parameters $\\theta^Q$ and $\\kappa^Q$ differ from $\\theta^P$ and $\\kappa^P$, respectively. The former are adjusted by risk premiums. Intuitively, it is correct to think that the former are risk premium adjusted expressions of the latter. E.g. if the long-run mean of the short rate was at 2\\%, it could be that bonds are priced as if the long-run rate was at 3\\%. This means the price is lower than if the future payoff was discounted by 2\\%. That extra reduction in price can be correctly interpreted as the result of a risk premium. That was to show you that the so called \"Q parameters\" are risk premium adjusted (and extracted from prices), while the $P$ parameters are extracted from empirical time-series data. I provide a formal and intuitive introduction into these asset pricing concepts in an upcoming MSc class on financial machine learning.\n",
    "\n",
    "For now, you need only to keep in mind that the parameters of the model are $\\kappa^P, \\theta^P$ for the short rate PDF and $\\theta^Q, \\kappa^Q$ for the bond prices. The parameter $\\sigma$ affects both, the short rate PDF and the bond price PDF.\n",
    "\n",
    "Note, prices are non-stationary! Hence, we work with continuousy compounded yields\n",
    "\\begin{align*}\n",
    "y_t(\\tau) &:= - \\frac{1}{\\tau} \\ln P_t(\\tau)  \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class OLS:\n",
    "\n",
    "    def __init__(self,X,y):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        :param X: np. matrix with X values\n",
    "        :param y: np.matrix with y values\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        return None\n",
    "\n",
    "    def runOLS(self):\n",
    "        \"\"\"\n",
    "        Computation of regression\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # compute regression beta and residuals\n",
    "\n",
    "\n",
    "        # calculate (x * x)^-1 with transosing and inverse functions\n",
    "        X_X_invers = (self.X.getT() * self.X).getI()\n",
    "\n",
    "        # calculate the beta ols estimate\n",
    "        self.beta_ols = X_X_invers * self.X.getT() * self.y\n",
    "        # print(self.beta_ols.shape)\n",
    "\n",
    "        # calcuate the residuals\n",
    "        self.resid = y - X * self.beta_ols\n",
    "\n",
    "\n",
    "        # calculate the volatility of the residual\n",
    "\n",
    "        # calculate the variance\n",
    "        var_res = 1 / (len(y) - len(self.beta_ols)) * self.resid.getT() * self.resid\n",
    "\n",
    "        # calcualte the standard error\n",
    "        self.vol_res = np.sqrt(var_res)\n",
    "\n",
    "\n",
    "        # standard errors and t-stat of beta estimates\n",
    "\n",
    "        # covariance matrix ov betas\n",
    "        var_b = var_res[0,0] * X_X_invers\n",
    "\n",
    "        # sd of betas laying on the diagonal of the covariance matrix\n",
    "        self.vol_b = np.sqrt(var_b.diagonal())\n",
    "\n",
    "        # TODO: Matrix checking\n",
    "\n",
    "        # check the dimensions\n",
    "        # beta as a 2 x 1 row vector\n",
    "        # print(self.beta_ols.shape)\n",
    "        # st must therefore also be a row vector for element wise division\n",
    "        # print(self.vol_b.shape)\n",
    "\n",
    "        # reshape\n",
    "        self.vol_b = self.vol_b.reshape(-1,1)\n",
    "\n",
    "        # calcualte the t stat\n",
    "        self.t_stat =  self.beta_ols / self.vol_b\n",
    "\n",
    "\n",
    "        # adjusted r^2\n",
    "\n",
    "        # calcuate the sample variance\n",
    "        help = y - y.mean()\n",
    "        var_y = (help.T * help) / (len(y) - 1)\n",
    "\n",
    "        # calcuate the adjusted r squared\n",
    "        self.adjust_r_squared = 1 - (var_res[0,0]/var_y[0,0])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def summaryStats(self):\n",
    "\n",
    "        # runOls\n",
    "        # self.runOLS()\n",
    "\n",
    "        # display results\n",
    "        results_df = pd.DataFrame(columns=[\"Betas\", \"vol Betas\",\"t-stat\"])\n",
    "\n",
    "        help_list1 = self.beta_ols.tolist()\n",
    "        help_list2 = self.vol_b.tolist()\n",
    "        help_list3 = self.t_stat.tolist()\n",
    "\n",
    "        help_list1 = [round(i[0],4) for i in help_list1]\n",
    "        help_list2 = [round(j[0],4) for j in help_list2]\n",
    "        help_list3 = [round(k[0],4) for k in help_list3]\n",
    "\n",
    "        results_df[\"Betas\"] = help_list1\n",
    "        results_df[\"vol Betas\"] = help_list2\n",
    "        results_df[\"t-stat\"] = help_list3\n",
    "\n",
    "        print(\"-\" * 10)\n",
    "        print(\"Summary Statistics:\")\n",
    "        print(results_df)\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"The residuals volatility is: {round(self.vol_res[0,0],4)}\")\n",
    "        print(f\"The adjusted R^2 is: {round(self.adjust_r_squared,4)}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "def set_time_index(df, timecolname):\n",
    "\n",
    "    \"\"\"This function sets the time col as index and makes sure it's a datetime object.\n",
    "\n",
    "    :param df: full Dataframe\n",
    "    :param timecolname: colname of the column that has time information in it\n",
    "    :return: full Dataframe\n",
    "    \"\"\"\n",
    "    # take the time column and convert it to a datetime object\n",
    "    df[timecolname] = pd.to_datetime(df[timecolname])\n",
    "\n",
    "    # set the index of the DF as the time Column\n",
    "    df.set_index(timecolname, inplace = True)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.1 Data\n",
    "\n",
    "You will work with the same interest rate data than a couple of weeks ago. The rates are in \"GovBondYields.xls\". \n",
    "\n",
    "Take the 12-, 60 and 120 months interest rates as your yields of interest, $y$. Take the 3-month yield as the short rate $r$.\n",
    "\n",
    "**Units:** Divide the rates data by 1200 to arrive at monthly values in decimals. Work with these units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# import the needed data\n",
    "df_bond = pd.read_excel(\"GovBondYields.xls\",sheet_name=\"Rates\",header=0)\n",
    "df_bond = set_time_index(df_bond,\"Date\")\n",
    "\n",
    "# print(df_bond.head())\n",
    "# print(df_bond.tail())\n",
    "# print(df_bond.describe())\n",
    "# plt.plot(df_bond)\n",
    "# plt.show()\n",
    "\n",
    "# Divide the rates by 1200 to arrive at monthly values\n",
    "df_bond = df_bond / 1200\n",
    "# print(df_bond.head())\n",
    "\n",
    "# get the names of the columns\n",
    "colnames = df_bond.columns.tolist()\n",
    "\n",
    "# get the wanted columns as pandas Series\n",
    "month_3_interst_r = df_bond[colnames[0]]\n",
    "month_12_interst = df_bond[colnames[2]]\n",
    "month_60_interst = df_bond[colnames[5]]\n",
    "month_120_interst = df_bond[colnames[7]]\n",
    "# print(month_3_interst_r.head())\n",
    "\n",
    "# get some intuition for r : monthly interest rate\n",
    "# print(month_3_interst_r.describe())\n",
    "# plt.plot(month_3_interst_r)\n",
    "# plt.show()\n",
    "\n",
    "# get the time horizon\n",
    "T = df_bond.shape[0]\n",
    "# print(T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.2 Financial Data Science Challenge\n",
    "\n",
    "Your data science task is to extract the time-series of the expected market price of risk that is priced into bonds. You can consider the latter to be the priced-in Expected Sharpe Ratio in bonds (expected risk premium per unit of risk). The precise formula is given below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G.3 Battle Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The battle plan is as follows. \n",
    "\n",
    "1. Write down the joint log likelihood function for $r_t$ and $y_t(\\tau), \\tau \\in [12,60,120]$. For that assume a pairwise independent Gaussian measurement error for all yields. Further assume each of these measurement errors shares the same volatility, which we call\n",
    "\\begin{equation}\n",
    "\\sigma_y.\n",
    "\\end{equation}\n",
    "Note, $\\sigma_y$ will be part of the parameter vector over which you optimize.\n",
    "\n",
    "Note, you can interpret observed yields as Vasicek (model)-implied yield $y_t(\\tau)$ plus a Gaussian measurement error that is drawn from $N(0,\\sigma^2_y)$.\n",
    "\n",
    "Note: the above assumption implies that the joint log likelihood function can be treated as the sum of 4 pairwise independent log likelihood functions. The independence refers to the respective residual being independent of the residuals of the other log likelihood. The parameters do of course affect all of the log likelihoods; although strictly speaking, $\\kappa^P, \\theta^P$ affect only the log likelihood for $r$, while $\\kappa^Q, \\theta^Q$ affect only the log likelhood for the 3 yields.\n",
    "\n",
    "2. Write a python function that implements the joint log likelihood function. More recent research has shown you could indeed fit the PDF for $r$ independently for the PDF of $y$. Yet, here, we optimize the joint log likelihood.\n",
    "\n",
    "3. Play a bit around with starting values for the optimization. That simple example reveals already, that you need to choose starting values carefully to end up in the global optimum. Regardless of your playing around, use at least the following procedure to find smart starting values\n",
    "\n",
    "3.1 fit $r$ to an OLS and recover the ML estimates for $\\kappa^P, \\theta^P, \\sigma$. Make these parameters be the respective starting values $\\kappa^P, \\theta^P, \\sigma$ in  the joint log likelihood optimization. \n",
    "\n",
    "3.2 for the starting values of the Q parameters, we assume a 0 risk premium. Hence $\\kappa^Q_{startValue} = \\kappa^P_{startValue}$ and $\\theta^Q_{startValue} = \\theta^P_{startValue}$. Double check: inside the optimization, Q and P parameters can differ(!). P parameters affect $r$ while $Q$ parameters affect yields (!)\n",
    "\n",
    "3.3 For the starting value of $\\sigma_y$ we do the following: Regress (OLS) the 60month yield onto $r$ and take the volatility of the residual as the starting value for $\\sigma_y$. \n",
    "\n",
    "\n",
    "4. The optimization routine is similar to the ipynb from class\n",
    "\n",
    "4.1 use sco.minimize\n",
    "\n",
    "4.2 use L-BFGS-B as the optimization routine\n",
    "\n",
    "4.3 use a tol of 1e-8\n",
    "\n",
    "4.5 minimize the negative joint log likelihood function\n",
    "\n",
    "4.6 the upper bound for all parameters is 100. \n",
    "\n",
    "4.7 the lower bound for all parameters is -100, except for $\\sigma$ and $\\sigma_y$ who lower bound we set to 0.000001.\n",
    "\n",
    "5. Estimate the model parameters of Vasicek.\n",
    "\n",
    "6. Inside the vasicek model, the priced-in market price of risk (Sharpe ratio) in bonds is\n",
    "\n",
    "\\begin{equation}\n",
    "RP_t = \\lambda_0 + \\lambda_1 \\times r_t\n",
    "\\end{equation}\n",
    "\n",
    "with\n",
    "\\begin{align*}\n",
    "\\lambda_0 &:= \\frac{\\kappa^P \\theta^P - \\kappa^Q \\theta^Q}{\\sigma_r} \\\\\n",
    "\\lambda_1 &:= \\frac{\\kappa^Q - \\kappa^P}{\\sigma_r}.\n",
    "\\end{align*}\n",
    "\n",
    "Here, you see that spread of Q and P parameters captures risk premium information. A formal explanation of that is not doable in the BSc course on Financial Data Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE: https://en.wikipedia.org/wiki/Vasicek_model\n",
    "\n",
    "\"Note simga stand vor volatility\"\n",
    "\n",
    "# implement the model\n",
    "def vasicek_model(rt,tau,k_q,o_q,sigma_r):\n",
    "\n",
    "    \"\"\" This function implements the calcualation of the implied yield\n",
    "    :param rt: pandas Series of the short rate rt\n",
    "    :param tau:  maturity in tau periods\n",
    "    :param k_q: param of model\n",
    "    :param o_q: param of model\n",
    "    :param sigma_r: parm of model\n",
    "    :return: model implied yield in a 1 x T numpy matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # convert rt to a numpy matrix : Loose the dates but usefull for further calculation\n",
    "    rt = np.matrix(rt)\n",
    "\n",
    "    # check the dimesnions : 1 x T : row vector\n",
    "    # print(rt.shape)\n",
    "\n",
    "    b_r = (1 - np.exp(-k_q*tau)) / k_q\n",
    "\n",
    "    a_r = (o_q -(sigma_r)/(2* k_q**2)) * (b_r - tau - sigma_r / (4*k_q) * b_r**2)\n",
    "\n",
    "    # price of a bond at time t with maturity tau\n",
    "    p_t = np.exp(a_r - b_r * rt)\n",
    "\n",
    "    # continuoisly compounded yields\n",
    "    y_t = (- 1/tau) * np.log(p_t)\n",
    "\n",
    "    # check the dimesnions : 1 x T : row vector\n",
    "    # print(y_t.shape)\n",
    "\n",
    "    return y_t\n",
    "\n",
    "# a = vasicek_model(rt = month_3_interst_r,tau = 120,k_q = 1,o_q = 1,sigma_r = 0.0001)\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-284018.6574564117\n",
      "-284018.6574564117\n"
     ]
    }
   ],
   "source": [
    "# we need the joint log likelyhood function of rt\n",
    "# Assumption: iid Gaussion white noise errors with et - N(0,sig_e^2)\n",
    "\n",
    "def joint_log_likelyhood_rt(rt,op,kappa,sigma_r):\n",
    "\n",
    "    \"\"\"THis function computes the jll of rt with a gaussian pdf\n",
    "    :param rt: pandas Series of the short rate rt\n",
    "    :param op: param of model, long run mean\n",
    "    :param kappa: param of model, speed of the reversion\n",
    "    :param sigma_r: param of model, instantaneous standard deviation of the interest rate shock\n",
    "    :return: joint log likelyhood of all r_t = (r_1,...,rT) : skalar\n",
    "    \"\"\"\n",
    "\n",
    "    # as is creates the same results\n",
    "\n",
    "    # shift the ts one day back\n",
    "    # rt_1 = rt.shift(1).dropna()\n",
    "    rt_1 = rt[0:T-1]\n",
    "\n",
    "    # convert rt to a numpy matrix\n",
    "    rt_1 = np.matrix(rt_1)\n",
    "\n",
    "    # remove the first datapoint from rt\n",
    "    rt = rt[1:T]\n",
    "\n",
    "    # convert rt to a numpy matrix\n",
    "    rt = np.matrix(rt)\n",
    "\n",
    "    # check the dimesnions : 1 x T : row vector\n",
    "    # print(rt.shape)\n",
    "\n",
    "    # calcuate the mu\n",
    "    # mu = op * (1-np.exp(-kappa)) + np.exp(-kappa) * rt[0]\n",
    "    # print(rt-mu)\n",
    "    mu = op * (1-np.exp(-kappa)) + np.exp(-kappa) * rt_1\n",
    "\n",
    "    # check the dimesnions : 1 x T : row vector\n",
    "    # print((rt-mu).shape)\n",
    "\n",
    "    # calcualte the sigma\n",
    "    sigma = sigma_r * ((1-np.exp(-2 * kappa))) / (2 * kappa)\n",
    "    # print(sigma)\n",
    "\n",
    "    # in matrix notation\n",
    "    log_joint_likely = - T/2 * np.log(2 * np.pi) - T/2 * np.log(sigma) - 1 / (2 * sigma) * (rt-mu) * (rt-mu).getT()\n",
    "\n",
    "    return float(log_joint_likely)\n",
    "\n",
    "a = joint_log_likelyhood_rt(rt = month_3_interst_r,op = 1,kappa = 1,sigma_r = 0.001)\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# we need the joint log likelyhood function of yt\n",
    "\n",
    "def joint_log_likelyhood_yt(yt,sigma_y,rt,tau,k_q,o_q,sigma_r):\n",
    "\n",
    "    \"\"\"THis function computes the jll of rt with a gaussian pdf\n",
    "    :param yt: pandas Series of the yield\n",
    "    :param sigma_y: instantaneous standard deviation of the interest rate shock\n",
    "    :param rt: pandas Series of the short rate rt\n",
    "    :param tau:  maturity in tau periods\n",
    "    :param k_q: param of model\n",
    "    :param o_q: param of model\n",
    "    :param sigma_r: parm of model\n",
    "    :return: joint log likelyhood of all y_t = (y_1,...,yT) : skalar\n",
    "    \"\"\"\n",
    "\n",
    "    # convert yt to a numpy matrix\n",
    "    yt = np.matrix(yt)\n",
    "\n",
    "    # check the dimesnions : 1 x T : row vector\n",
    "    # print(yt.shape)\n",
    "\n",
    "    # calcuate the mu\n",
    "    # you can interpret observed yields as Vasicek (model)-implied yield plus a Gaussian measurement error\n",
    "    mu = vasicek_model(rt,tau,k_q,o_q,sigma_r)\n",
    "    # print(mu)\n",
    "\n",
    "    # check the dimesnions : 1 x T : row vector\n",
    "    # print(mu.shape)\n",
    "\n",
    "    # check the dimesnions : 1 x T : row vector\n",
    "    # print((yt-mu).shape)\n",
    "\n",
    "    # calcualte the sigma\n",
    "    sigma = sigma_y\n",
    "\n",
    "    # in matrix notation\n",
    "    log_joint_likely = - T/2 * np.log(2 * np.pi) - T/2 * np.log(sigma) - 1 / (2 * sigma) * (yt -mu) * (yt-mu).getT()\n",
    "\n",
    "    return float(log_joint_likely)\n",
    "\n",
    "# a = joint_log_likelyhood_yt(yt = month_120_interst,sigma_y = 0.0001,rt = month_3_interst_r,tau = 120,k_q = 1,o_q = 1,sigma_r = 0.0001)\n",
    "# print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_likelyhood_yt_rt(param_vec):\n",
    "\n",
    "    \"\"\"This function computes the joint likelyhood function\n",
    "    :param op: parm of model\n",
    "    :param kappa: parm of model\n",
    "    :param o_q: parm of model\n",
    "    :param k_q: parm of model\n",
    "    :param sigma_y: parm of model\n",
    "    :param sigma_r: parm of model\n",
    "    :return: negative log likelyhood with given unknown parameters: skalar\n",
    "    \"\"\"\n",
    "\n",
    "    # order of the parm vector\n",
    "    # [op,kappa,o_q,k_q,sigma_y,sigma_r]\n",
    "\n",
    "    # get the parameters\n",
    "    op = param_vec[0]\n",
    "    kappa = param_vec[1]\n",
    "    o_q = param_vec[2]\n",
    "    k_q = param_vec[3]\n",
    "    sigma_y = param_vec[4]\n",
    "    sigma_r = param_vec[5]\n",
    "\n",
    "    # IDEA: joint log likelihood function can be treated as the sum of 4 pairwise independent log likelihood functions\n",
    "\n",
    "    # function for the short rate\n",
    "    L_r = joint_log_likelyhood_rt(rt =month_3_interst_r,op = op,kappa = kappa,sigma_r = sigma_r)\n",
    "    print(L_r)\n",
    "    L_y_12 = joint_log_likelyhood_yt(yt = month_12_interst,sigma_y =sigma_y ,rt = month_3_interst_r,tau = 12,k_q = k_q,o_q = o_q,sigma_r = sigma_r)\n",
    "    print(L_y_12)\n",
    "    L_y_60 = joint_log_likelyhood_yt(yt =month_60_interst,sigma_y =sigma_y ,rt = month_3_interst_r,tau = 60,k_q = k_q,o_q = o_q,sigma_r = sigma_r)\n",
    "    print(L_y_60)\n",
    "    L_y_120 = joint_log_likelyhood_yt(yt =month_120_interst,sigma_y =sigma_y ,rt = month_3_interst_r,tau = 120,k_q = k_q,o_q = o_q,sigma_r = sigma_r)\n",
    "    print(L_y_120)\n",
    "    # compute the total joint likelyhood function\n",
    "    L = L_r + L_y_12 + L_y_60 + L_y_120\n",
    "\n",
    "    # return the negative log likelyhood\n",
    "    return L * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-284018.6574564117\n",
      "-258211.46627526206\n",
      "-297070.9561077101\n",
      "-302054.62736152345\n",
      "1141355.7072009072\n",
      "-284018.6574564117\n",
      "-258211.46627526206\n",
      "-297070.9561077101\n",
      "-302054.62736152345\n",
      "1141355.7072009072\n"
     ]
    }
   ],
   "source": [
    "# parameters as default: are getting overwritten\n",
    "op = 1\n",
    "kappa = 1\n",
    "o_q = 1\n",
    "k_q = 1\n",
    "sigma_y = 0.001\n",
    "sigma_r = 0.001\n",
    "\n",
    "# all in one vector\n",
    "param_vec = [op,kappa,o_q,k_q,sigma_y,sigma_r]\n",
    "\n",
    "L = joint_likelyhood_yt_rt(param_vec)\n",
    "print(L)\n",
    "\n",
    "# get the length: number of unknown parameters\n",
    "K = len(param_vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Summary Statistics:\n",
      "    Betas  vol Betas    t-stat\n",
      "0  0.0001     0.0000    2.1704\n",
      "1  0.9852     0.0064  152.9405\n",
      "----------\n",
      "The residuals volatility is: 0.0004\n",
      "The adjusted R^2 is: 0.9741\n",
      "----------\n",
      "6.890460011518503e-05\n",
      "0.9852150542451509\n",
      "0.00037794053136844924\n",
      "----------\n",
      "Summary Statistics:\n",
      "    Betas  vol Betas    t-stat\n",
      "0  0.0001     0.0000    2.1704\n",
      "1  0.9852     0.0064  152.9405\n",
      "----------\n",
      "The residuals volatility is: 0.0004\n",
      "The adjusted R^2 is: 0.9741\n",
      "----------\n",
      "6.890460011518503e-05\n",
      "0.9852150542451509\n",
      "0.00037794053136844924\n"
     ]
    }
   ],
   "source": [
    "# find the respective starting parameters op, kappa, sigma_r: we need to retrieve 3 values\n",
    "\n",
    "# op is the long-run mean of r, All future trajectories of r will evolve around a mean level b in the long run: intercept\n",
    "# kappa is the speed of mean reversion, characterizes the velocity at which such trajectories will regroup around {\\displaystyle b}b in time : slope\n",
    "# sigma_r is the instantaneous standard deviation of the interest rate shock : variance of residuals\n",
    "\n",
    "# make use of the AR(1) structure\n",
    "\n",
    "df_for_regression_r = pd.DataFrame(columns = [\"month_3\"])\n",
    "df_for_regression_r[\"month_3\"] = df_bond[colnames[0]]\n",
    "\n",
    "# get the y column: exclude the first x values\n",
    "y = df_for_regression_r[\"month_3\"][1:T]\n",
    "\n",
    "# convert to numpy matrix\n",
    "y = np.matrix(y)\n",
    "\n",
    "# need to transpose the vector\n",
    "y = y.reshape(T-1,1)\n",
    "\n",
    "# create X : Lag 1\n",
    "lag1 = df_for_regression_r[\"month_3\"][0:T-1]\n",
    "\n",
    "# create X : Intercept\n",
    "ones = np.ones(T-1)\n",
    "\n",
    "# combine to a dataframe\n",
    "x_df = pd.DataFrame(ones)\n",
    "x_df[\"Lag1\"] = lag1.values\n",
    "\n",
    "# convert to a numpy matrix\n",
    "X = np.matrix(x_df)\n",
    "\n",
    "# create the model\n",
    "model = OLS(X,y)\n",
    "\n",
    "# fit the model\n",
    "model.runOLS()\n",
    "\n",
    "# get the results\n",
    "model.summaryStats()\n",
    "\n",
    "\"Care adjust the values: See the PDF Formula\"\n",
    "# y = a+ bx + e\n",
    "# with a = o_p * (1 -ek_p )\n",
    "# with b = e -k_p\n",
    "# with e =  sig2 * 1-....\n",
    "\n",
    "# intercept\n",
    "op = float(model.beta_ols[0])\n",
    "print(op)\n",
    "\n",
    "\n",
    "# slope\n",
    "kappa = float(model.beta_ols[1])\n",
    "print(kappa)\n",
    "\n",
    "\n",
    "# resid\n",
    "sigma_r  = float(model.vol_res)\n",
    "print(sigma_r)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# find the respective starting parameters o_q,k_q\n",
    "\n",
    "# starting with zero risk premium, thus:\n",
    "o_q = op\n",
    "k_q = kappa"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Summary Statistics:\n",
      "    Betas  vol Betas   t-stat\n",
      "0 -0.0008     0.0001  -9.9178\n",
      "1  0.9638     0.0139  69.3643\n",
      "----------\n",
      "The residuals volatility is: 0.0008\n",
      "The adjusted R^2 is: 0.8852\n",
      "----------\n",
      "0.0007958932001805073\n",
      "----------\n",
      "Summary Statistics:\n",
      "    Betas  vol Betas   t-stat\n",
      "0 -0.0008     0.0001  -9.9178\n",
      "1  0.9638     0.0139  69.3643\n",
      "----------\n",
      "The residuals volatility is: 0.0008\n",
      "The adjusted R^2 is: 0.8852\n",
      "----------\n",
      "0.0007958932001805073\n"
     ]
    }
   ],
   "source": [
    "# find the respective starting parameters sigma_y\n",
    "\n",
    "df_for_regression_y = pd.DataFrame(columns = [\"month_3\",\"month_120\"])\n",
    "df_for_regression_y[\"month_3\"] = df_bond[colnames[0]]\n",
    "df_for_regression_y[\"month_60\"] = df_bond[colnames[5]]\n",
    "\n",
    "y = df_for_regression_y[\"month_3\"]\n",
    "\n",
    "# convert to numpy matrix\n",
    "y = np.matrix(y)\n",
    "\n",
    "# need to transpose the vector\n",
    "y = y.reshape(T,1)\n",
    "\n",
    "# create X : Intercept\n",
    "ones = np.ones(T)\n",
    "\n",
    "# combine to a dataframe\n",
    "x_df = pd.DataFrame(ones)\n",
    "x_df[\"month_60\"] = df_for_regression_y[\"month_60\"].values\n",
    "\n",
    "# convert to a numpy matrix\n",
    "X = np.matrix(x_df)\n",
    "\n",
    "# create the model\n",
    "model = OLS(X,y)\n",
    "\n",
    "# fit the model\n",
    "model.runOLS()\n",
    "\n",
    "# get the results\n",
    "model.summaryStats()\n",
    "\n",
    "# resid\n",
    "sigma_y  = float(model.vol_res)\n",
    "print(sigma_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((-100.0, 100.0), (-100.0, 100.0), (-100.0, 100.0), (-100.0, 100.0), (1e-06, 100.0), (1e-06, 100.0))\n",
      "((-100.0, 100.0), (-100.0, 100.0), (-100.0, 100.0), (-100.0, 100.0), (1e-06, 100.0), (1e-06, 100.0))\n"
     ]
    }
   ],
   "source": [
    "# now start with the numerical optimazation of the negative joint likelihood function\n",
    "\n",
    "# starting values\n",
    "starting_v = [op,kappa,o_q,k_q,sigma_y,sigma_r]\n",
    "\n",
    "# lower bounds\n",
    "lb = -100 * np.ones(K)\n",
    "lb[K-2] = 0.000001\n",
    "lb[K-1] = 0.000001\n",
    "\n",
    "# upper bounds\n",
    "ub = 100 * np.ones(K)\n",
    "\n",
    "# bounds:\n",
    "bounds = tuple((lb[x],ub[x]) for x in range(0,K))\n",
    "print(bounds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2129.558003928598\n",
      "1645.4665564904894\n",
      "1642.3458710229163\n",
      "1641.5349099245236\n",
      "2129.558067334117\n",
      "1645.4665564904894\n",
      "1642.3458710229163\n",
      "1641.5349099245236\n",
      "2129.558005757024\n",
      "1645.4665564904894\n",
      "1642.3458710229163\n",
      "1641.5349099245236\n",
      "2129.558003928598\n",
      "1645.4665892719927\n",
      "1642.345912505934\n",
      "1641.5349533662168\n",
      "2129.558003928598\n",
      "1645.4665564861677\n",
      "1642.3458710354116\n",
      "1641.5349099396985\n",
      "2129.558003928598\n",
      "1645.4627584046423\n",
      "1642.3421121464285\n",
      "1641.5311612372282\n",
      "2129.5502042263142\n",
      "1645.4665396039766\n",
      "1642.3458496541555\n",
      "1641.5348875468214\n",
      "-623945870511542.0\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-623945870386747.5\n",
      "-inf\n",
      "-inf\n",
      "-inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-aa95eb067276>:31: RuntimeWarning: divide by zero encountered in log\n",
      "  y_t = (- 1/tau) * np.log(p_t)\n",
      "c:\\users\\win10\\pycharmprojects\\elasticstack\\venv\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:557: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x) - f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-623945870449147.5\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-623945870511542.0\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-623945870511542.0\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-623945870511542.0\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-617768188625238.9\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "2129.558003928598\n",
      "1645.4665564904894\n",
      "1642.3458710229163\n",
      "1641.5349099245236\n",
      "2129.558067334117\n",
      "1645.4665564904894\n",
      "1642.3458710229163\n",
      "1641.5349099245236\n",
      "2129.558005757024\n",
      "1645.4665564904894\n",
      "1642.3458710229163\n",
      "1641.5349099245236\n",
      "2129.558003928598\n",
      "1645.4665892719927\n",
      "1642.345912505934\n",
      "1641.5349533662168\n",
      "2129.558003928598\n",
      "1645.4665564861677\n",
      "1642.3458710354116\n",
      "1641.5349099396985\n",
      "2129.558003928598\n",
      "1645.4627584046423\n",
      "1642.3421121464285\n",
      "1641.5311612372282\n",
      "2129.5502042263142\n",
      "1645.4665396039766\n",
      "1642.3458496541555\n",
      "1641.5348875468214\n",
      "      fun: -7058.905341366528\n",
      " hess_inv: <6x6 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-6.34055177e+03, -1.82842631e+02, -1.17706213e+04, -2.33485479e+00,\n",
      "        1.13056496e+06,  7.86033526e+05])\n",
      "  message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 21\n",
      "      nit: 1\n",
      "     njev: 3\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([6.89046001e-05, 9.85215054e-01, 6.89046001e-05, 9.85215054e-01,\n",
      "       7.95893200e-04, 3.77940531e-04])\n",
      "2129.558003928598\n",
      "1645.4665564904894\n",
      "1642.3458710229163\n",
      "1641.5349099245236\n",
      "2129.558067334117\n",
      "1645.4665564904894\n",
      "1642.3458710229163\n",
      "1641.5349099245236\n",
      "2129.558005757024\n",
      "1645.4665564904894\n",
      "1642.3458710229163\n",
      "1641.5349099245236\n",
      "2129.558003928598\n",
      "1645.4665892719927\n",
      "1642.345912505934\n",
      "1641.5349533662168\n",
      "2129.558003928598\n",
      "1645.4665564861677\n",
      "1642.3458710354116\n",
      "1641.5349099396985\n",
      "2129.558003928598\n",
      "1645.4627584046423\n",
      "1642.3421121464285\n",
      "1641.5311612372282\n",
      "2129.5502042263142\n",
      "1645.4665396039766\n",
      "1642.3458496541555\n",
      "1641.5348875468214\n",
      "-623945870511542.0\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-623945870386747.5\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-623945870449147.5\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-623945870511542.0\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-623945870511542.0\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-623945870511542.0\n",
      "-inf\n",
      "-inf\n",
      "-inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-aa95eb067276>:31: RuntimeWarning: divide by zero encountered in log\n",
      "  y_t = (- 1/tau) * np.log(p_t)\n",
      "c:\\users\\win10\\pycharmprojects\\elasticstack\\venv\\lib\\site-packages\\scipy\\optimize\\_numdiff.py:557: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x) - f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-617768188625238.9\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "2129.558003928598\n",
      "1645.4665564904894\n",
      "1642.3458710229163\n",
      "1641.5349099245236\n",
      "2129.558067334117\n",
      "1645.4665564904894\n",
      "1642.3458710229163\n",
      "1641.5349099245236\n",
      "2129.558005757024\n",
      "1645.4665564904894\n",
      "1642.3458710229163\n",
      "1641.5349099245236\n",
      "2129.558003928598\n",
      "1645.4665892719927\n",
      "1642.345912505934\n",
      "1641.5349533662168\n",
      "2129.558003928598\n",
      "1645.4665564861677\n",
      "1642.3458710354116\n",
      "1641.5349099396985\n",
      "2129.558003928598\n",
      "1645.4627584046423\n",
      "1642.3421121464285\n",
      "1641.5311612372282\n",
      "2129.5502042263142\n",
      "1645.4665396039766\n",
      "1642.3458496541555\n",
      "1641.5348875468214\n",
      "      fun: -7058.905341366528\n",
      " hess_inv: <6x6 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-6.34055177e+03, -1.82842631e+02, -1.17706213e+04, -2.33485479e+00,\n",
      "        1.13056496e+06,  7.86033526e+05])\n",
      "  message: 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 21\n",
      "      nit: 1\n",
      "     njev: 3\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([6.89046001e-05, 9.85215054e-01, 6.89046001e-05, 9.85215054e-01,\n",
      "       7.95893200e-04, 3.77940531e-04])\n"
     ]
    }
   ],
   "source": [
    "# optimazation\n",
    "\n",
    "# returns the optimized values\n",
    "mle_estimate = scipy.optimize.minimize(fun = joint_likelyhood_yt_rt,x0 = starting_v,bounds = bounds, method = \"L-BFGS-B\",tol = 1e-8)\n",
    "\n",
    "print(mle_estimate)\n",
    "\n",
    "# get the results as an array\n",
    "optimize_res = mle_estimate.x\n",
    "# print(optimize_res)\n",
    "# print(joint_likelyhood_yt_rt(optimize_res) * -1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def get_priced_in_risk(rt,optimize_res):\n",
    "\n",
    "    \"\"\"This function calculates the  priced-in market price of risk\n",
    "    :param rt: pandas Series of pandas Series of the short rate rt\n",
    "    :param optimize_res: array with optimized values\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # convert rt to a numpy matrix\n",
    "    # rt = np.matrix(rt)\n",
    "\n",
    "    # check the dimesnions : 1 x T : row vector\n",
    "    # print(rt.shape)\n",
    "\n",
    "    # get the estimated values\n",
    "    op = optimize_res[0]\n",
    "    kappa = optimize_res[1]\n",
    "    o_q = optimize_res[2]\n",
    "    k_q = optimize_res[3]\n",
    "    sigma_r = optimize_res[K-1]\n",
    "\n",
    "    # needed help formulas : skalars\n",
    "\n",
    "    # is positive and large\n",
    "    lam0 = (kappa * op - k_q * o_q) / sigma_r\n",
    "    print(lam0)\n",
    "\n",
    "    # is negative and big\n",
    "    lam1 = (k_q - kappa) / sigma_r\n",
    "    print(lam1)\n",
    "\n",
    "    # sharpe ratio\n",
    "    rtp = lam0 + lam1 * rt\n",
    "\n",
    "    return rtp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "Date\n",
      "1954-04-01    0.0\n",
      "1954-05-01    0.0\n",
      "1954-06-01    0.0\n",
      "1954-07-01    0.0\n",
      "1954-08-01    0.0\n",
      "             ... \n",
      "2005-12-01    0.0\n",
      "2006-01-01    0.0\n",
      "2006-02-01    0.0\n",
      "2006-03-01    0.0\n",
      "2006-04-01    0.0\n",
      "Name: 3, Length: 625, dtype: float64\n",
      "0.0\n",
      "0.0\n",
      "Date\n",
      "1954-04-01    0.0\n",
      "1954-05-01    0.0\n",
      "1954-06-01    0.0\n",
      "1954-07-01    0.0\n",
      "1954-08-01    0.0\n",
      "             ... \n",
      "2005-12-01    0.0\n",
      "2006-01-01    0.0\n",
      "2006-02-01    0.0\n",
      "2006-03-01    0.0\n",
      "2006-04-01    0.0\n",
      "Name: 3, Length: 625, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "priced_in_risk = get_priced_in_risk(month_3_interst_r,optimize_res)\n",
    "print(priced_in_risk)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}