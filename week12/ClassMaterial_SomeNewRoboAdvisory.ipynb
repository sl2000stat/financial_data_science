{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dear Students,\n",
    "\n",
    "we cover the following topics in the section on \"New Developments in  Robo Advisory \":\n",
    "    \n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**1. Why do rule-based strategies overperform the unconstrained Tangency Portfolio?**  \n",
    "\n",
    "**2. What is industry referring to when they talk about Factor Investing?**  \n",
    "    \n",
    "**3. A Cross-Sectional approach to estimate unconditional factor premiums.**  \n",
    "\n",
    "**4. High Level Introduction to Harvesting Risk Premia**\n",
    "    \n",
    "**5. Handling large set of factors: PCA and Gram Schmidt.**\n",
    "        \n",
    "        \n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "I have planned to share **code snippets** on the following topics\n",
    "\n",
    "C1. **PCA** on EU and US Government Bond Yields\n",
    "\n",
    "C2. When to use **Markowitz** and when not to use it.\n",
    "\n",
    "**Yet, for now, I have stepped back as I fear it would lead to unnecessary stress in the weeks prior to the exams.**\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "I have planned to assign the following problem set: Is the **Fama-MacBeth** approach powerful enough to detect non zero unconditional factor premiums in euro area equity returns?  **Yet, for now, I have stepped back as I fear it would lead to unnecessary stress in the weeks prior to the exams.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Online Lectures\n",
    "\n",
    "I hope you will find it suitable that I refer you to your personalized YT Playlist with the title \"BSc: Financial Data Science\". \n",
    "\n",
    "https://www.youtube.com/playlist?list=PLyQSjcv8LwAF0V_3k9wZnN0BF5i9qLW64\n",
    "\n",
    "The videos are ordered, I am sure you have noticed that. Hence, all of the videos after the GARCH session are of relevance, except for the video with the title NDAM_V2_HowToAccountForHigherMomentRiskWhenBuildingOptimal Portfolios. I skip that one. On the other hand, I have added some videos on a non-supervised learning technique called principal component analyis. I have added that as I think the median student will like to be exposed to that standard data science technique.\n",
    "\n",
    "As I have spent an extra week on the MLE for the Vasicek model, I will not be able to talk about VAR models. In short: a vector AR model (VAR) is estimated by applying OLS to every dimension of the VAR. The notes here do only once mention VAR (when talking about Fama-MacBeth and the I-CAPM). I am happy to distribute some notes if you feel it is helpful.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Rule-based Investment Strategies and their Relation to Markowitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Popular Rule-Based Portfolio Strategies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "The following rule-based strategies are all special mean-variance portfolios.\n",
    "\n",
    " \n",
    " \n",
    "- 'market weight' portfolio: mimicks the relative market capitalization of each asset within the investors investment opportunity set. \n",
    "\n",
    "\n",
    "- 'diversity weight' portfolio: applies a specific power transformation to the 'market weights'. \n",
    "\n",
    "- 'equal weight' portfolio: 'one over N' rule.  \n",
    "\n",
    "- 'Risk-Parity' portfolio:  chooses weights proportional to the inverse of either variance or volatility.  \n",
    "\n",
    "- 'Minimum Variance' portfolio:  the left most point on the Mean-Variance Efficient Frontier.\n",
    "\n",
    "\n",
    "- 'Equal Risk Contribution' portfolio:  constructs weights such that each asset contributes equally to the total portfolio variance.  \n",
    "\n",
    "\n",
    "- 'Kelly Rule' portfolio: weights such that the portfolio's expected log return is maximized. \n",
    "\n",
    "\n",
    "- 'Proportional to Sharpe Ratio' portfolio: invests relative to the realized past 5-year Sharpe ratios of each asset class in the portfolio. \n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "## 1.2 Horse Race \n",
    "\n",
    "How do these rule-based portfolio strategies perform relative to a mean-variance optimal portfolio?  \n",
    "\n",
    "* Barcap U.S. Treasuries,\n",
    "* Barcap U.S. Credit, \n",
    "* S\\&P 500,\n",
    "* MSCI EAFE. \n",
    "\n",
    "\n",
    "- monthly data from January 1978 to December 2011. \n",
    "\n",
    "- A strategy that is implemented at time $t$ relies on data over the past five years \n",
    "\n",
    "- Portfolios are held for one month and new portfolios are formed at the end of each month.\n",
    "\n",
    "- One-month T-bill is   the risk-free rate. \n",
    "\n",
    "- Shorting  is restricted to $-100\\%$.\n",
    "\n",
    "\n",
    "Findings:\n",
    "\n",
    "\n",
    "* Any diversified portfolios has produced a higher Sharpe ratio than a 100\\% investment in U.S. equities only.\n",
    "\n",
    "\n",
    "* Unconstrained mean-variance optimal portfolio underperforms massively   \n",
    "\n",
    "\n",
    "* Equal-weight portfolio (simple and robust) is hard to beat\n",
    "\n",
    "\n",
    "* 'Risk Parity' performed even better than the 'Equal Weight portfolio'   \n",
    "\n",
    "\n",
    "* 'Minimum-Variance Portfolio' performed very well  \n",
    "\n",
    " \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "## 1.3 Reason for Underperformance of Mean-Variance Optimal Portfolios \n",
    "\n",
    "- 'Garbage-in and garbage-out' problem \n",
    "\n",
    "\n",
    "\n",
    "- 'Minumum-Variance Portfolio':  does not rely on  expected return's.  \n",
    "\n",
    "\n",
    "- 'risk parity portfolio':   does not rely on expected returns nor on pairwise correlations.  \n",
    "\n",
    "\n",
    "- 'Equal Weight portfolio':  all assets are treated as equally attractive  \n",
    "\n",
    "\n",
    "Punchline:  strategies that rely on few estimation inputs perform better than complex models that are theoretically optimal on the one hand, but which require a large amount of estimates as inputs. \n",
    " \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Factor Investing: An Industry Perspective, blended with FE Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Norway's Sovereign Wealth Fund\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "- One of the largest sovereign wealth funds.\n",
    "\n",
    "- Norway citizens understand  that high returns can only be earned in the long-run if one invests into systematic risks\n",
    "\n",
    "- And an event like the 2008/09 financial crisis was the realisation of a risky event, for which one earns a premium in the long-run.\n",
    "\n",
    "\n",
    "**Advice:** \n",
    " \n",
    "* In the future, risk premiums could be collected more cheaply through passive investment devices like Exchange Traded Funds. \n",
    "\n",
    "\n",
    "* The fund should move beyond 'static risk factors' such as fixed-income and equity investments to collect additional risk premiums.\n",
    "\n",
    "\n",
    "* It should invest into dynamic risk factors, which involve dynamic trading and long-short positions. E.g. value/growth, momentum, short volatility strategies.\n",
    "\n",
    "\n",
    "* And finally: every asset class is nothing else than a unique combination of risk factors. Hence, asset allocation should not focus on asset classes, but directly on risk factors.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Smart Beta\n",
    "\n",
    "\n",
    "Typical examples for overperforming subindices are \n",
    "\n",
    "\n",
    "* Value stocks and value-growth premium\n",
    "\n",
    "\n",
    "* Momentum stocks and 'momentum premium' \n",
    "\n",
    "\n",
    "* 'Illiquidity' and 'illiquidity premium'  \n",
    "\n",
    "\n",
    "* 'Credit' and 'credit risk premium'. \n",
    "\n",
    "\n",
    "* 'Short vol' and 'short volatility risk premium'. \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**How to invest into  smart beta strategies?** \n",
    "\n",
    "-  long and short positions, which results in a dynamic portfolio strategy\n",
    "\n",
    " \n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**A trustworthy 'smart beta' risk premium factor should fulfill the following four conditions**\n",
    "\n",
    "\n",
    "* be justified by academic research\n",
    "\n",
    "\n",
    "* exhibit a significant factor premium that is expected to persist in the future\n",
    "\n",
    "\n",
    "* have a sufficiently long return history to quantify its 'bad times' with subsequent maximum drawdowns\n",
    "\n",
    "\n",
    "* be tradeable via liquid exchange traded instruments.\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "**Note:**\n",
    "- Rational Rationale: As in the CAPM, a factor premium is positive if the average investor does not like to hold that particular risk factor. A factor premium is negative if the average investor likes having that factor in his portfolio. \n",
    "\n",
    "\n",
    "- Behavioral Rationale: There is also a significant share of academic studies that argue that behavioral biases are the cause behind several factor premiums.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Factor Pricing Models\n",
    "\n",
    "\n",
    "Factor models such as the CAPM, Fama/French, APT, as well as smart beta strategies, such as low vol, and others, do share one common logic. Their expected return and risk relationship stems from a linear factor pricing model. Such models are also called 'beta pricing models'.  \n",
    "\n",
    "\n",
    " \n",
    "### 2.3.1 Single Factor Beta Model\n",
    "\n",
    "\\begin{align*}\n",
    "E[r_i] - r_f &= \\underbrace{ \\lambda }_{\\text{MPR}} \\; \\times \\; \\underbrace{ \\frac{cov(f,r_i)}{var(f)} }_{\\text{factor risk exposure}} \\\\\\\\\n",
    "    & \\equiv   \\lambda \\; \\times \\; \\beta,\n",
    "\\end{align*}\n",
    "\n",
    "where $MPR$ is   the 'market price of risk'; and $ \\frac{cov(f,r_i)}{var(f)}$ quantifies the amount of factor risk that is present in asset $i$. \n",
    "\n",
    "\n",
    "\n",
    "If  \n",
    "\n",
    "$$\n",
    "f \\equiv r_M - r_f\n",
    "$$\n",
    "\n",
    "with the factor risk premium\n",
    "\n",
    "$$\n",
    "\\lambda \\equiv E[f]\n",
    "$$\n",
    "\n",
    "you end up wit the Security Market Line of the CAPM, i.e.\n",
    "\n",
    "$$\n",
    "E[r_i] - r_f = \\left(E[r_M]-r_f\\right) \\, \\times \\, \\frac{cov(r_M-r_f, r_i)}{var(r_M-r_f)}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "   \n",
    "\n",
    "### 2.3.2 Multi-Factor Beta Model\n",
    "\n",
    " \n",
    "\n",
    "\\begin{align}\n",
    "E[r_i] -r_f &= \\underbrace{ \\lambda'}_{1\\times k} \\; \\times \\; \\underbrace{\\Sigma^{-1}_F}_{k \\times k} \\;\\times \\; \\underbrace{ \\text{Cov}(F,r_i)}_{k \\times 1}\n",
    "\\end{align}\n",
    "\n",
    "which we can write more compactly as\n",
    "\n",
    "$$\n",
    "E[r_i] -r_f = \\underbrace{\\lambda'}_{\\text{MPR}} \\; \\times \\; \\underbrace{\\beta}_{\\text{factor risk exposure }}.\n",
    "$$\n",
    "\n",
    " \n",
    "* $\\Sigma_F$ is an invertible covariance matrix of the factor risk premium factors, $F = (f_1, ..., f_k)$\n",
    "\n",
    "\n",
    "* $\\lambda'$ is the  row vector of market prices of factor risk.  \n",
    "\n",
    "\n",
    "* Each element of $\\lambda$ can be negative or positive, i.e. $\\lambda \\in \\mathcal{R}^k$ \n",
    "\n",
    " \n",
    "\n",
    "* $\\text{Cov}(F,r_i)$ is a column vector. The m-th element coincides with $cov(f_m,r_i)$ and quantifies how much of the mth factor risk is present in asset $i$. \n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- **time-series factor model**:  $\\{F_t\\}_t$ is observed   \n",
    "\n",
    "- **cross-sectional factor model**: $\\beta_{F,r_j}$ is observed \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Classical Factor Models\n",
    "\n",
    "\n",
    "**CAPM:**\n",
    "\n",
    "$$\n",
    "f \\equiv r_M - r_f.\n",
    "$$\n",
    "\n",
    " \n",
    "$$\n",
    "\\lambda = E[r_M] - r_f.\n",
    "$$\n",
    " \n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Fama-French 3-factor model**:   \n",
    "\n",
    "\\begin{align}\n",
    "f_1 & \\equiv r_M - r_f \\\\\n",
    "f_2 & \\equiv r_{HML} \\\\\n",
    "f_3 & \\equiv r_{SIZE}.\n",
    "\\end{align}\n",
    "\n",
    " \n",
    "$$\n",
    "\\lambda_F = \\begin{pmatrix} E[r_M]-r_f \\\\ E[r_{HML}] \\\\ E[r_{SIZE}] \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Intertemporal CAPM of Merton, or I-CAPM**\n",
    "\n",
    "\\begin{align*}\n",
    "E[r_i] - r_f &\\overbrace{=}^{ICAPM} \\lambda_M \\, \\times \\, \\beta_{i,M} \\, + \\, \\sum_{l=1}^K \\, \\lambda_{l} \\times \\beta_{i,l}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\lambda_M \\equiv E[r_M]-r_f$, $\\beta_{i,M} \\equiv \\frac{cov(r_M-r_f,r_i)}{var(r_M-r_f)}$, $\\lambda_l$ is the price of risk for innovations in risk factor $l$ and $\\beta_{i,l}$ is the normalized amount of co-movement of asset $i$'s return with innovations in risk factor 'l'. Remember, any factor that affects the evolution of the level or slope of the Capital Allocation Line is part of the set $l=\\{1,...,K\\}$. \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "### 2.3.4 How to incorporate non-tradeable risk factors\n",
    "\n",
    "**Case 1:** I-CAPM \n",
    "\n",
    "-  expected to work well if the non-tradeable risk factors are factors that drive the level or slope of the Capital Allocation Line \n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Case 2:** 'factor mimicking portfolio'\n",
    "\n",
    "- run a linear regression of your non-tradeable risk factor on a bunch of tradeable asset returns. The resulting projection coincides with the trade-able portfolio that has maximial correlation with the non-trade-able risk factor.  \n",
    "\n",
    "- Assume risk factor $f_j$ is not tradeable, while all other $f_l$ for $l \\neq j$ are tradeable risk factors. We want to substitute $f_j$ with its factor mimicking portfolio. Hence, we regress $f_j$ onto a vector of tradeable returns $R_*$, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "f_j = \\gamma + \\beta \\, R_* + \\epsilon, \\quad \\epsilon \\perp R_*.\n",
    "\\end{align*}\n",
    "\n",
    "Let's define the projection of $f_j$ on $R_*$ as $x$, i.e\n",
    "\n",
    "\\begin{align*}\n",
    "x &:= \\gamma + \\beta \\, R_*,\n",
    "\\end{align*}\n",
    "\n",
    "which in other words stands for the return on the tradeable portfolio that best mimicks the non-tradeable risk factor $f_j$.\n",
    "\n",
    "\n",
    "\n",
    "In order to get risk exposure of an asset $i$ to the non-tradeable risk factor $f_j$, we approximate $cov(r_i,f_j)$ with $cov(r_i,x)$, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    " cov(r_i,x) &= \\beta cov(r_i,R_*).\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "In order to get the market price of risk for the non-tradeable factor $f_j$, we compute\n",
    "\n",
    "$$\n",
    "[\\lambda]_{j,1} := \\gamma + \\beta E[ R_*] - r_f\n",
    "$$\n",
    "\n",
    "The resulting linear multi-factor beta pricing model equals\n",
    "\n",
    "\\begin{align*}\n",
    "E[r_i] - r_f &= [\\lambda'   \\times   \\Sigma^{-1}_F]_j \\, \\times\\, \\beta cov(r_i,R_*)\\, + \\, \\sum_{l \\neq j} [\\lambda' \\times \\Sigma^{-1}_F]_l \\, cov(r_i,f_l),\n",
    "\\end{align*}\n",
    "\n",
    "where for all $l \\neq j$\n",
    "\n",
    "$$\n",
    "[\\lambda]_{l,1} = E[f_l]  \n",
    "$$\n",
    "\n",
    "if $f_l$ is a tradeable excess return\n",
    "\n",
    "\n",
    "\n",
    "or\n",
    "\n",
    "\n",
    "$$\n",
    "[\\lambda]_{l,1} = E[f_l]  -r_f\n",
    "$$\n",
    "\n",
    "if $f_l$ is a tradeable return.\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Difference between Case 1 and Case 2:**\n",
    "\n",
    "- key difference is in the calculation of the market price of the non-tradeable factor\n",
    "\n",
    "- ICAPM approach uses the Fama-MacBeth procedure \n",
    "\n",
    "- Factor mimicking portfolio approach relies on the expected return of the linear factor mimicking portfolio. \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Despite that, I-CAPM approach is grounded in economic theory. In contrast, the portfolio mimicking approach exploits correlation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Risk Management with Factor Models\n",
    "\n",
    "\n",
    "\n",
    "Modern risk management firms like 'Barra MSCI' provide risk management tools that build on the factor model logic. The underlying idea goes back to the CAPM, which derived that (i) total risk of an asset consists of systematic risk and diversifiable idiosyncratic risk and (ii) the latter converges to zero if that asset is part of a sufficiently large portfolio. \n",
    "\n",
    "\n",
    "\n",
    "For simplicity, \\textbf{assume} $F$ is a matrix of excess returns with dimension $k \\times T$.\n",
    "\n",
    "$$\n",
    "\\rightarrow \\; \\lambda \\equiv \\underbrace{E[F]}_{k \\times 1}.\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "If the asset market follows a linear Factor model with $F$ as the factor premiums, then the expected risk premium of an asset $i$ would coincide with\n",
    "\n",
    "\\begin{align*}\n",
    "E[r_i] - r_f &= \\underbrace{E[F]'}_{1 \\times k} \\, \\times\\, \\Sigma^{-1}_F \\, \\text{Cov}(\\underbrace{F}_{k \\times T}, \\underbrace{r_i}_{T \\times 1}).\n",
    "\\end{align*}\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Realized risk premiums of asset $i$, $r_i$, deviate from expected risk premiums $E[r_i] - r_f$, i.e.\n",
    "\n",
    "$$\n",
    "r_{i,t} - r_{f,t-1} = F_t' \\, \\times \\, \\Sigma^{-1}_F \\,\\times\\, \\text{Cov}(F,r_i) + \\epsilon_{i,t} \n",
    "$$\n",
    "\n",
    "where $\\epsilon_{i,t}$ is the idiosyncratic return innovation of asset $i$ and $F_t$ is the $k \\times 1$ column vector of realized factor premiums. Note, both $\\epsilon_{i,t}$ and $F_t - E_{t-1}[F_t]$ are unforecastable as of $\\mathcal{F}_{t-1}$, and hence, innovations. Innovations are also called 'news', meaning that unforecastable information arrived in $\\mathcal{F}_t$ and asset markets reacted to that news, so realized returns differ from expectations. \n",
    "\n",
    "\n",
    "Formally, return innovations coincide with \n",
    "\n",
    "$$\n",
    "\\underbrace{r_{i,t} - E_{t-1}[r_{i,t}]}_{\\text{risk}} = \\underbrace{\\left( F_t - E_{t-1}[F_t] \\right)' \\, \\times \\, \\Sigma^{-1}_F \\, \\text{Cov}(F,r_i)}_{\\text{systematic r.}} \\; + \\; \\underbrace{\\epsilon_{i,t}}_{\\text{idiosync.r.}}\n",
    "$$\n",
    "\n",
    "Factor risk $F_t - E_{t-1}[F_t]$ is unpredictable. The covariance of asset $i$'s return with the factor premiums quantify asset $i$'s risk contribution to the well diversified portfolio. The ratio of $\\epsilon_{i,t}$'s variance over the variance of $r_{i,t}$ measures the reduction in asset $i$'s risk when it is held in a well diversified portfolio, instead of a standalone basis.\n",
    "\n",
    "\n",
    "There are specialized risk firms in the market that provide customers with time-series data of all types of risk factors. Risk managers can use that information to decompose their portfolio risk into systematic and idiosyncratic risk. Systematic risk can further be decomposed into the contribution of each risk factor. \n",
    "\n",
    "\n",
    "In its most general form, there are time indices on many of the above mentioned quantities. For example, the market price of risk $\\lambda$ is likely time-varying, i.e.\n",
    "\n",
    "$$\n",
    "\\lambda_{t-1} \\equiv E_{t-1}[F_t].\n",
    "$$ \n",
    "\n",
    "Asset $i$'s factor risk exposure is likely also time-varying\n",
    "\n",
    "$$\n",
    "\\beta_{i,t-1} \\equiv \\Sigma^{-1}_{F,t-1} \\times Cov_{t-1}(F_t, r_{i,t}). \n",
    "$$\n",
    "\n",
    "That time-variation arises from either stochastic volatility in the factor premiums and/or in a time-varying correlation between asset $i$ and the factor premiums. Measuring these conditional forecasts requires good data and good econometrics and machine learning.\n",
    "\n",
    "\n",
    "In order to keep notation at a minimum, I usually do not carry the time indices around. \n",
    "\n",
    "\n",
    "Another key application for linear factor models in risk management is to compute the covariance matrix of a large asset universe. Prior to the use of factor models, the covariance matrix of returns was calculated using past data\n",
    "\n",
    "\\begin{align*}\n",
    "[\\hat{\\Sigma}_{r}]_{[j,k]} = \\frac{1}{T-K} \\, \\sum_{t=1}^T \\, (\\tilde{r}_{j,t} - \\text{mean}(\\tilde{r}_{j}))(\\tilde{r}_{k,t} - \\text{mean}(\\tilde{r}_{k})), \\quad j,k \\in \\{1,...,K\\},\n",
    "\\end{align*}\n",
    "\n",
    "where $\\tilde{r}$ is the return in excess of the risk-free rate. \n",
    "\n",
    "\n",
    "For $K=1000$ assets, one had to estimate 500500 entries to the covariance matrix. While that estimate is unbiased, it has a very high variance. That high variance reduces the precision of the estimates of each entry to the covariance matrix.\n",
    "\n",
    "\n",
    "Linear factor models reduced the number of estimations that were necessary to pin down the covariance matrix of returns.\n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{\\hat{\\Sigma}_r}_{K \\times K} &\\overbrace{:=}^{LFM} \\hat{\\beta}' \\, \\times \\, \\hat{\\Sigma}_F \\, \\times \\, \\hat{\\beta} + \\hat{\\Sigma}_{\\eta}\n",
    "\\end{align*}\n",
    "\n",
    " where $\\hat{\\beta}$ is a GLS estimate for the factor exposure matrix. It is determined by projecting time-series return data onto realized factor risk premiums. $\\hat{\\Sigma}_F$ is an estimate of the low-dimensional factor risk premium panel. $\\hat{\\Sigma}_{\\eta}$ is a diagonal matrix of idiosyncratic risks. One way to compute that matrix is as follows: \n",
    " \n",
    "$$\n",
    "\\hat{\\Sigma}_{\\eta} = diag( \\frac{1}{T-K} \\hat{\\epsilon}^2_{1,GLS}, ...,  \\frac{1}{T-K} \\hat{\\epsilon}^2_{K,GLS}).\n",
    "$$ \n",
    "\n",
    "where $\\hat{\\epsilon}^2_{1,GLS}$ is the squared residual from regressing the return of asset $1$ onto all $K$ risk factor premiums.\n",
    "\n",
    "\n",
    "The linear factor estimate to the covariance matrix of returns is biased but has lower variance (ergo higher precision) than the sample covariance matrix. Remember, the linear factor model estimate for the covariance matrix is accurate if all correlations between two assets arise from exposure to common risk factors, i.e. \n",
    "\n",
    "$$\n",
    "cov(\\tilde{r}_j, \\tilde{r}_m) = (\\beta_{[:,j]})' \\; \\times \\; \\Sigma_F \\; \\times \\; \\beta_{[:,m]}.\n",
    "$$\n",
    "\n",
    "The takeaway is that risk managers have to (i) incorporate all relevant systematic risk factors, (ii) use a proper statistical model for their times-series dynamics, and (iii) ensure that the resulting residuals are indeed uncorrelated across assets. Making a mistakes in one of these steps is likely leading to wrong estimates of the covariance matrix.\n",
    "\n",
    "\n",
    "Let's put some numbers on the table. For 1000 assets and 10 risk factors, one needs 11055 estimates to pin down the $1000 \\times 1000$ return covariance matrix. This is factor 50 lower compared to the sample covariance matrix. The break-down is as follows:\n",
    "\n",
    "* 1000 estimates for firm specific risk\n",
    "* 55 estimates for the sample covariance matrix of factor risk premiums\n",
    "* 10000 estimates for $\\beta$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cross-Sectional Estimation of Expected Factor Premiums \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "Fama/MacBeth (1973) propose a method to jointly estimate the market price of factor risk $\\lambda$ and the amount of factor risk exposure $\\beta$, i.e.\n",
    "\n",
    "$$\n",
    "E[r_i] - r_f = \\lambda \\; \\beta_i,\\quad \\forall i \\{1,...,I\\}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "## 3.1 Fama-MacBeth for a One-Factor Model \n",
    " \n",
    "\n",
    "\n",
    "* Step 1: Ask yourself whether there is an economic theory that supports the factor model  \n",
    "\n",
    "\n",
    "* Step 2: Collect a panel of asset excess returns $\\{r_{i,t}\\}_{i=1,t=1}^{i=I,t=T}$ and a time-series of the risk factor, here for the CAPM as an example it would be: $\\{r_{M,t}-r_{f,t}\\}_{t=1}^{t=T}$\n",
    "\n",
    "\n",
    "* Step 3: Time-series regression to get each stocks 'beta'. \n",
    "\n",
    " \\begin{align*}\n",
    "        & \\text{for all stocks} \\; i \\in \\{1,...,I\\} \\; \\text{run the following time-series regression:} \\\\\n",
    "        & \\begin{pmatrix} r_{i,t=1} \\\\ .... \\\\ r_{i,t=T}  \\end{pmatrix} = \\begin{pmatrix} 1 & r_{M,t=1}-r_{f,t=0} \\\\ ... & ... \\\\ 1 & r_{M,t=T}-r_{f,t=T-1} \\end{pmatrix} \\begin{pmatrix} \\beta_{i,0} \\\\ \\beta_{i,1} \\end{pmatrix} + \\begin{pmatrix} \\epsilon_{i,t=1} \\\\ ... \\\\ \\epsilon_{i,t=T} \\end{pmatrix}\n",
    "    \\end{align*}\n",
    "    \n",
    "   to recover $\\{\\hat{\\beta_i}\\}_{i=1}^{I}$. If the data supports the OLS assumptions, you can rely on the OLS estimates.         Otherwise, us Generalized Least Squares. \n",
    "    \n",
    "    \n",
    "* Step 4: Treat $\\{\\hat{\\beta_i}\\}_{i=1}^{K}$ as observed and run for each time period a cross-sectional regression.  \n",
    "\n",
    " \\begin{align*}\n",
    "          & \\text{for all} \\; t \\in \\{1,...,T\\} \\; \\text{run the following cross-sectional regression:} \\\\\n",
    "          &\\begin{pmatrix} r_{i=1,t} \\\\ .... \\\\ r_{i=I,t}  \\end{pmatrix} = \\begin{pmatrix} 1 & \\hat{\\beta}_{i=1,1} \\\\ ... & ... \\\\ 1 & \\hat{\\beta}_{i=I,1} \\end{pmatrix} \\begin{pmatrix} \\alpha_{t} \\\\ \\lambda_{t} \\end{pmatrix} + \\begin{pmatrix} e_{i=1,t} \\\\ ... \\\\ e_{i=I,t} \\end{pmatrix}\n",
    "        \\end{align*}\n",
    "        \n",
    "   which allows you to get a least-squares estimate for \n",
    "   \n",
    "$$\n",
    "[ \\hat{\\alpha}_{t=1}, \\, , ..., \\, \\hat{\\alpha}_{t=T} ] \\quad \\quad \\text{and} \\quad   [ \\hat{\\lambda}_{t=1}, \\, ... , \\, \\hat{\\lambda}_{t=T} ].\n",
    "$$\n",
    "        \n",
    "        \n",
    "* Step 5: Fama/MacBeth define the risk premium for market exposure (factor premium for a unit exposure to market risk, $E[r_M]-r_f$) as\n",
    "\n",
    "$$\n",
    "\\hat{\\lambda}_{\\text{FMacB}} := \\frac{1}{T} \\sum_{t=1}^T \\hat{\\lambda}_{t}.\n",
    "$$\n",
    "        \n",
    "        \n",
    "* Step 6: Test whether $\\hat{\\lambda}_{\\text{FMacB}}$ is statistically different from zero, using the t-stat, i.e.\n",
    "\n",
    "$$\n",
    "t_{\\lambda} := \\frac{\\hat{\\lambda}_{\\text{FMacB}}}{ std(\\hat{\\lambda_t}) / \\sqrt{T}},\n",
    "$$\n",
    "\n",
    " where $std(\\hat{\\lambda})$ is the sample standard deviation of the estimated $\\{ \\hat{\\lambda}_{t}\\}_{t=1}^T$.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2  Fama-MacBeth for I-CAPM   \n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "\\begin{align*}\n",
    "E[r_{i,t} -r_{f,t-1}] &= \\alpha_i + \\beta_{i,M} E\\left[ r_{M,t} - r_{f,t-1}\\right] + \\begin{pmatrix} \\beta_{i,\\text{dy}} \\\\ \\beta_{i,\\text{Term}} \\\\ \\beta_{i,\\text{Def}} \\\\ \\beta_{i,\\text{Rf}} \\end{pmatrix}'  \\begin{pmatrix} \\lambda_{u^\\text{dy}} \\\\ \\lambda_{u^\\text{Term}} \\\\ \\lambda_{u^\\text{Def}} \\\\ \\lambda_{u^\\text{rf}}\\end{pmatrix},\n",
    "\\end{align*}\n",
    "\n",
    "where $u^\\text{dy}, \\, u^\\text{Term}, u^\\text{Def}, \\, u^\\text{rf}$ stands for innovations in the dividend yield, term spread, default spread, risk-free rate, respectively. The term $\\lambda_{u^\\text{Term}} $ stands for the <u>market price of term spread risk</u>. The respective $\\beta$ coefficients, e.g. $\\beta_{i,\\text{Term}}$, stand for the factor exposure of asset $i$.  \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. High Level Intro into Harvesting Risk Premia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Static Risk Premiums  in Equity\n",
    "\n",
    "- Dimson, Marsh and Staunton (2010)  argue that for 1900-2009, the equity risk premium in excess of cash (bonds) was 5.2\\% (4.2\\%) for U.S. and 4.0\\% (3.8\\%) in the rest of the 18 countries (dollar denominated). \n",
    "\n",
    "\n",
    "**Notice: Expected vs Realized Excess Returns**\n",
    "\n",
    "- Expected risk premium differs from the average realized excess return **!**\n",
    "\n",
    "- Studies show that the latter is a very poor predictor of the former. \n",
    "\n",
    "- Reason: expected returns are time-varying and  realized excess returns are strongly affected by unexpected price movements. \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Notice: CAPM: Does it explain expected returns or does it not?**\n",
    "\n",
    "-  CAPM  fails to explain why low beta stocks outperformed high beta stocks; giving rise to the notion of 'betting against beta'. \n",
    "\n",
    "- Buss and Vilkov (2012, RFS) argue that historical data produce inaccurate estimates of forward-looking betas. When using option data to extract CAPM betas, the authors find a positive relationship between average excess returns and beta. \n",
    "\n",
    "- Recent research also finds that using high-frequency data to calculate CAPM betas leads to a positive Security Market Line and drives out the significance of Fama-French's value and size factor. For more on that you could read Hollstein, Prokopczuk and Simen (2019, Management Science).\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "### 4.1.1 Extracting Expected (Static) Equity Premium via Valuation Models\n",
    "\n",
    "\n",
    "\n",
    "The Gordon model implies that the expected equity risk premium equals the current dividend yield plus expected dividend growth, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "E[r_i] -r_f &= \\frac{D^i}{P^i} + g^i,\n",
    "\\end{align*}\n",
    "\n",
    "So, if asset $i$ has currently a dividend yield of 5\\% and an expected dividend growth rate of 2\\%, the model implied equity risk premium would be 7\\%.\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "### 4.1.2 Static Bond Premium\n",
    "\n",
    "\n",
    "- The bond risk premium in the Government bond market is also called term premium.\n",
    "\n",
    "- It quantifies the additional return that long-duration bonds pay over short-term bonds.\n",
    "\n",
    "- The slope of the yield curve is a noisy signal of the bond premium. \n",
    "\n",
    "- This is because it consists of two parts: (i) the change in the expected path of future short-term interest rates and (ii) the actual bond premium. Notice, both channels induce offsetting investment implications. It is hence important to tell these components apart. The usual approach to separate both parts is via an arbitrage-free term structure model.  \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "### 4.1.3 Static Credit Premium\n",
    "\n",
    "-  Dimson, Marsh and Staunton (2002) use data from 1900 to 2000 and find that Investment grade defaultable bonds have only marginally outperformed US government bonds. \n",
    "\n",
    "- The margin has been roughly 0.2-0.5\\% per year. \n",
    "\n",
    "- In general, the spread between yields of defaultable and non-defaultable bonds compensate the investor for the expected loss due to default or downgrade **and** for a credit risk premium.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Dynamic Risk Premiums\n",
    "\n",
    "\n",
    "\n",
    "The empirical finance literature has identified numerous dynamic risk premium strategies. I provide a very short overview for 4 such strategies: value, carry, momentum and volatility.\n",
    "\n",
    "\n",
    "**Value**: \n",
    "\n",
    "- Stocks with strong fundamentals relative to their market price, have on average outperformed growth stocks.\n",
    "\n",
    "- More current research has also shown that the value premium is not a US only phenomenon but rather a global phenomenon that holds across asset classes.\n",
    "\n",
    "- There have been periods where Value strongly underperformed Growth. \n",
    "\n",
    "\n",
    "- There are different economic theories to rationalize the value premium. One set of theories argues that the value premium is the result of behavioral biases. Another strand argues it is based on value firms being exposed to more systematic risk\n",
    " \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Momentum:** \n",
    "\n",
    "- buy recent winners and sell recent losers.\n",
    "\n",
    "- Such a strategy has performed well on a single stock basis and across several asset markets.\n",
    "\n",
    "- Momentum strategies on a single stock basis are also called trend-following strategy.\n",
    "\n",
    "- One prominent economic explanation  is again based on behavioral biases.  \n",
    "\n",
    "- large transaction costs. \n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Carry:** \n",
    "\n",
    "- Over the past 50 years, buying high yielding currency and shorting low yielding currency has been a profitable trade.\n",
    "\n",
    "- Standard macro theory postulates that any yield differential across currencies should be offset by future currency depreciation. In practice, the opposite has happened on average; namely, high yielding currencies appreciated and hence became even more lucrative for the early movers. \n",
    "\n",
    "\n",
    "- Carry trades payed high Sharpe ratios within G10 countries, and performed even better with emerging market countries. \n",
    "\n",
    "- A major set-back happend during the financial crisis. \n",
    "\n",
    "- Escalator strategy  \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**Selling Vol:** \n",
    "\n",
    "- Escalator strategy that collects premiums in good times and that faces tremendous losses during market turmoil. \n",
    "\n",
    "- Coincides with writing call and put options.\n",
    "\n",
    "- Realized volatility in the stock market is roughly 2\\% to 4\\% lower than the priced-in expected volatility in the option market. The spread between both can be interpreted as a volatility risk premium. This premium is negative as the average investor likes to hold an option to protect his wealth from unexpected crashes. He is therefore willing to pay for such an insurance, which coincides with the risk premium for the seller of volatility. \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "- Spread between realized variance in the stock market and priced-in expectd variance in the option market is close to zero for single stock options.\n",
    "\n",
    "- Yet, it is large for index options\n",
    "\n",
    "- Driessen, Maenhout and Vilkov (2008) explain this with a correlation risk premium (i.e. diversification vanishes in times when you need it most)   \n",
    "\n",
    "- A trading strategy that goes directly after the correlation risk premium is a strategy that sells an option on the index and buys single stock options of all constituents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Factor Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The anomalies literature is the scientific foundation for the quantitative asset management industry. Since the mid-1990s, factor-based ETFs have gained tremendous popularity. By mid-2016, these funds managed approximately 1.35 trillion US dollars in the US market; accounting for roughly 10\\% of the market capitalization of traded securities.\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    " \n",
    "Return anomalies are return patterns that appear to pay excess returns that are above the fair rate for the respective amount of systematic risk. Popular anomalies are Momentum, Value-vs-Growth, Investment, Profitability, Intangibles and several Trading Frictions.\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$ \n",
    "\n",
    "The 2018 RFS publication of Hou, Xue and Zhang with the title 'Repcliating Anomalies' replicate 447 published return anomalies from the finance and accounting literature. These authors find that 85\\% of these anomalies cannot be replicated when\n",
    "\n",
    "* requiring a t-stat of larger than 3\n",
    "\n",
    "\n",
    "* relying on portfolio sorts that go long the top decile portfolio and shorts the bottom decile portfolio\n",
    "\n",
    "\n",
    "* computing value-weighted returns of the decile portfolio instead of equal-weight returns\n",
    "\n",
    "\n",
    "* controlling for macro-cap stocks that have in total a market value of less than 3\\% but account for more than 60\\% of all stocks. \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Only very few factor anomalies survive the bar of  Hou, Xue and Zhang\n",
    "\n",
    " \n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "What shall we take away from the factor-zoo with more fake than real animals?\n",
    "\n",
    "* I treat all anomalies as a data artefact. I rather over-reject than being fooled by p-hacking, data mining and data noise.\n",
    "\n",
    "\n",
    "* Competitive markets pay, hopefully a premium for systematic risk; hence beta. Why should simple long-short strategies earn alpha? Significant amounts of cash jump onto the band waggon and make any existing alpha disappear. That is what we should anticipate to happen. \n",
    "\n",
    "\n",
    "* The following thought is painful for researchers. Incentive-wise, there is an incentive for p-hacking, data-mining etc: just think about Well-paid consulting jobs and tenure offers from prestiguous universities. A discussion of these incentive misalignments can be found in  Hou, Xue and Zhang. \n",
    "\n",
    "\n",
    "* Factor investing in the sense of going long-short stocks based on some empirical characteristic is nothing I would build a pension portfolio on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Handling Big Data with PCA and Gram-Schmidt Orthogonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Gram-Schmidt Orthogonalization\n",
    "\n",
    "Gram-Schmidt is a popular projection method. It allows data scientists to keep interpretability in factors and to detect and subsequentially control multi-collinearity problems in the feature matrix $X$.\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Assume you have found a set of risk premium factors $F$. Empirically, these factor premiums share a substantial cross-correlation among each other, which creates numerical instabilities and ambiguous interpretations. I therefore advice, you orthogonalize the risk factors. \n",
    "\n",
    " \n",
    "\\begin{align*}\n",
    "G &:= \\left(\\text{chol}(\\Sigma_F)\\right)^{-1} \\, \\times \\, \\left(F - E[F]\\right),\n",
    "\\end{align*}\n",
    "\n",
    "where $chol(\\Sigma_F)$ is the lower triangular Cholesky decomposition of the covariance matrix $\\Sigma_F$ and $G$ is called the 'Gram-Schmidt orthogonalization of F'. \n",
    "\n",
    "\n",
    "\n",
    "$G$ has the following intuitive features: The first risk premium factor in $G$ is simply the 'z-score' of the first $F$ risk premium factor. This says it is demeaned and has a unit variance. The second risk premium factor in $G$ contains only the information of the second $F$ risk premium factor that is new, relative to the first risk premium factor of $F$ and $G$. The third risk premium factor in $G$ contains information of the third risk premium factor of $F$ that is orthogonal to the first and second risk premium factors of $F$ and $G$.f\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "I hope it is helpful to visualize that intuition using the concept of linear projection:  \n",
    "\n",
    "\\begin{align*}\n",
    "g_1 &:= \\frac{\\tilde{g}_1}{||\\tilde{g}_1||} \\quad \\; \\tilde{g}_1 \\equiv \\tilde{f}_1 \\\\\\\\\n",
    "g_2 &:= \\frac{\\tilde{g}_2}{||\\tilde{g}_2 ||} \\quad \\; \\tilde{g}_2 \\equiv \\tilde{f}_2 - \\text{proj}_{\\tilde{g}_1} \\, \\tilde{f}_2 \\\\\\\\\n",
    "g_3 &:= \\frac{\\tilde{g}_3}{||\\tilde{g}_3 ||} \\quad \\; \\tilde{g}_3 \\equiv \\tilde{f}_3 - \\text{proj}_{\\tilde{g}_1} \\, \\tilde{f}_3 - \\text{proj}_{\\tilde{g}_2} \\, \\tilde{f}_3\\\\\\\\\n",
    ".... & .... \\\\\n",
    "g_k &:= \\frac{\\tilde{g}_k}{||\\tilde{g}_k ||} \\quad \\; \\tilde{g}_k \\equiv \\tilde{f}_k - \\sum_{i=1}^{k} \\text{proj}_{\\tilde{g}_i} \\, \\tilde{f}_k \n",
    "\\end{align*}  \n",
    " \n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Now, the question pops up: what does the linear factor model look like if we use the Gram-Schmidt factors? Well, I want you to verify that it looks as follows\n",
    "\n",
    "\\begin{align*}\n",
    "E[r_i]-r_f = \\left((chol(\\Sigma_F))^{-1} \\lambda_F\\right)' \\, \\times \\, Cov(G,r_i)\n",
    "\\end{align*}  \n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 PCA\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "A Principal Component Analysis is one way to back out a set of risk factors that are responsible for most of the unpredictable return movements in the underlying factors. Such risk factors are also called 'statistical risk factors'. \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Eigenvalue Decomposition of Symmetric Matrices\n",
    "\n",
    "\n",
    " \n",
    "Assume your portfolio consists of $K$ assets. Let $\\Sigma$ be the corresponding $K \\times K$ covariance matrix. We borrow from Linear Algebra the insight that a symmetric matrix can be re-written as  \n",
    "\n",
    "\\begin{align}\n",
    "\\Sigma & \\equiv E \\, \\Lambda \\, E'\n",
    "\\end{align}\n",
    "\n",
    "where $E$ is the $K \\times K$ eigenvector matrix and $\\Lambda$ is the $K\\times K$ diagonal matrix of eigenvalues.\n",
    "\n",
    "\n",
    "Each column of $E$ contains one eigenvector $e$, i.e.\n",
    "\n",
    "$$\n",
    "E \\equiv [e_1, ..., e_K] \\quad \\quad \\text{matrix of K eigenvectors}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "and each entry on the main diagonal of $\\Lambda$ is an Eigenvalue $\\lambda$, i.e.\n",
    "\n",
    "$$\n",
    "\\Lambda = diag(\\lambda_1, ..., \\lambda_K) \\quad \\quad \\text{diagonal matrix of eigenvalues}.\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "  \n",
    "### 4.2.2 Inverting Symmetric Matrices\n",
    "\n",
    "First, remember, you cannot invert $\\Sigma_X$ if one of the main-diagonal elements of $\\Lambda$ is zero. Mathematically, this means\n",
    "\n",
    "\\begin{align}\n",
    "\\Sigma^{-1} &= \\left(E \\; \\times \\Lambda \\; \\times E' \\right)^{-1} \\\\\n",
    "&= (E')^{-1} \\; \\times \\; (\\Lambda)^{-1} \\; \\times \\; (E)^{-1} \\\\\n",
    "&= E \\; \\times \\; (\\Lambda)^{-1} \\; \\times \\; E',\n",
    "\\end{align}\n",
    "\n",
    "which highlights that the inversion of a covariance matrix does not exist if at least one of its eigenvalues is zero.\n",
    "\n",
    "\n",
    "Second, I recommend you always sort $\\Lambda$ and $E$ in descending order. This means the largest eigenvalue is ordered first and the first column of the eigenvector matrix coincides with its eigenvector.\n",
    "\n",
    "\n",
    "Third, I suggest, you drop each eigenvector that corresponds to a zero and close to zero eigenvalue. The 'pseudo'-inverse would then be\n",
    "\n",
    "\\begin{align}\n",
    "\\text{pseudo}(\\Sigma^{-1}) &:= [E]_{[1:K,1:\\bar{K}]} \\; \\times \\; \\left([\\Lambda]_{[1:\\bar{K}, 1:\\bar{K}]} \\right)^{-1} \\; \\times \\; \\left([E]_{[1:K,1:\\bar{K}]} \\right)'\n",
    "\\end{align}\n",
    "\n",
    "where $0<\\bar{K} \\leq K$ coincides with the number of non-zero eigenvalues of the covariance matrix. Note, that $\\text{pseudo}(\\Sigma^{-1}) $ has again dimension $K \\times K$.\n",
    "\n",
    "\n",
    "Finally, make sure you know the largest and smallest eigenvalue of the matrix you try to invert. If their ratio gets too large, also called 'the **condition number** gets too large', then your matrix is collinear in a numerical sense. The collinearity exists in a math sense if the ratio is infinity.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 PCA via an Example\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "$$\n",
    "\\underbrace{X}_{T \\times K} = \\left[\\underbrace{r^{(1)}}_{T \\times 1}, ..., r^{(K)} \\right] \\quad \\quad \\text{:K time-series of stock returns }\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**First**,  \n",
    "\n",
    "$$\n",
    "{X}^{dm} = \\left[r^{(1)}-\\bar{r}_1 \\times I_{T \\times 1}, ..., r^{(K)}-\\bar{r}_K \\times I_{T \\times 1} \\right] \\quad \\quad \\text{:K time-series of demeand stock returns}, \n",
    "$$\n",
    "\n",
    "where $\\bar{r}_{j}, j \\in [1,...,K]$ is the sample mean of stock $j$'s return.\n",
    "\n",
    "\n",
    "**Second**, \n",
    "\n",
    "\\begin{align}\n",
    "\\Sigma &:= E[(X-E[X])' (X - E[X])] \\\\\n",
    "&= \\frac{1}{T} \\, {X^{dm}}' \\; {X^{dm}}.\n",
    "\\end{align}\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    " \n",
    "$$\n",
    "\\Sigma \\equiv E \\; \\Lambda \\; E' \\quad \\quad \\quad \\text{:Eigenvalue decomposition of cov-matrix} \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "**The Goal of PCA is to Rotate $X^{dm}$ into a matrix $\\tilde{X}^{dm}$, such that the resulting Covariance Matrix of $\\tilde{X}^{dm}$ is diagonal**. \n",
    "\n",
    "\n",
    "Mathematically, this means\n",
    "\n",
    "\\begin{align}\n",
    "\\underbrace{\\tilde{X}}_{T \\times K} &= \\left[\\underbrace{\\tilde{x}^{(1)}}_{T \\times 1}, ..., \\tilde{x}^{(K)} \\right] \\quad \\quad \\quad \\text{:A linear rotation of X} \\\\ \\nonumber \\\\\n",
    "{\\tilde{X}}^{dm} &:= \\left[\\tilde{x}^{(1)}-\\bar{\\tilde{x}}^{(1)}\\times I_{T\\times 1}, ..., \\tilde{x}^{(K)}-\\bar{\\tilde{x}}^{(K)}\\times I_{T \\times 1} \\right] \\\\ \\nonumber \\\\\n",
    "& \\text{such that} \\\\ \\nonumber \\\\\n",
    "\\tilde{\\Sigma} &:= \\frac{1}{T} \\, \\tilde{X}^{dm'} \\; \\tilde{X}^{dm} \\quad \\quad \\quad \\text{:is diagonal}.  \n",
    "\\end{align}\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Let's look at the **Solution of that rotation problem:**\n",
    "\n",
    "\n",
    " \n",
    "\\begin{align}\n",
    "\\underbrace{\\tilde{X}^{dm}}_{T \\times K} &:= \\underbrace{{X}^{dm}}_{T \\times K} \\; \\times \\; \\underbrace{P}_{K \\times K} \\quad \\quad \\text{:linear rotation}. \n",
    "\\end{align}\n",
    "\n",
    "$$\n",
    "\\\\ \n",
    "\\\\\n",
    "$$\n",
    "**So, which $P$ shall we choose? Clearly, we have a degree of freedom here. But we also have the constraint that the covariance matrix of $\\tilde{X}$ shall be diagonal**. So, lets write: \n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{\\Sigma} &:= \\frac{1}{T} \\; (\\tilde{X}^{dm})' \\; \\tilde{X}^{dm} \\\\\n",
    "&=  \\frac{1}{T} \\; \\left(\\underbrace{{X}^{dm}}_{T \\times K} \\; \\times \\; \\underbrace{P}_{K \\times K}\\right)' \\; \\underbrace{{X}^{dm}}_{T \\times K} \\; \\times \\; \\underbrace{P}_{K \\times K} \\\\\n",
    "&= P' \\; \\Sigma \\; P.\n",
    "\\end{align}\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "As $\\Sigma$ has its own eigenvalue decomposition, i.e. $\\Sigma = E \\Lambda E'$, so we end up with\n",
    "\n",
    "$$\n",
    "\\tilde{\\Sigma} = P' \\; \\left[ E \\, \\Lambda \\, E' \\right] \\; P.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Hence, we need to choose $P$ such that $\\tilde{\\Sigma}$ is diagonal. One popular choice is to set\n",
    "\n",
    "$$\n",
    "P \\; \\overbrace{\\equiv}^{PCA}  \\; E,\n",
    "$$\n",
    "\n",
    "because\n",
    "\n",
    "* First, $E$ is an orthonormal matrix\n",
    "\n",
    "\n",
    "* Second, choosing $E$ to be the rotation matrix, makes $\\tilde{\\Sigma}$ be diagonal. Maybe surprisingly, $\\tilde{\\Sigma}$ will not just be diagonal, but each of its diagonal elements does coincide with one of the eigenvalues of the original covariance matrix, i.e.\n",
    "\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\tilde{\\Sigma} \\; \\overbrace{=}^{PCA} \\;  \\Lambda.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Said differently, the matrix of statistical risk factors equals\n",
    "\n",
    "$$\n",
    "\\tilde{X}^{dm} = X^{dm} \\times E.\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Practical Tips when working with PCA\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "**First**, if $X^{dm}$ is the demeand matrix of the original $T \\times K$ data, then \n",
    "\n",
    "$$\n",
    "\\tilde{X}^{dm} := X^{dm} \\times sort_d(E)\n",
    "$$\n",
    "\n",
    "represents the matrix of risk factors, or principal components. Notice, the 'd' index highlights you've sorted the eigenvalue and eigenvector matrices in descending order.\n",
    "\n",
    "\n",
    "**Second**, to explicitly highlight the relation to PCs, I highlight their relation\n",
    "\n",
    "$$\n",
    "\\tilde{X}^{dm} = \\begin{pmatrix} PC1_1 & ... & PCK_1 \\\\ ... & ... & ... \\\\ PC1_T & ... & PCK_T\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Note, the first column is the time-series of the most important risk factor while the last column represents the time-series of the least important risk factor.\n",
    "\n",
    "\n",
    "**Third**,\n",
    "\n",
    "$$\n",
    "Cov(\\tilde{X}^{dm}) = \\Lambda.\n",
    "$$\n",
    "\n",
    "**Fourth**,\n",
    "\n",
    "$$\n",
    "\\frac{\\lambda_i}{\\sum_{j=1}^K \\lambda_j}\n",
    "$$\n",
    "\n",
    "quantifies the percentage of variance of $X^{dm}$ that is captured by the i-th principal component.\n",
    "\n",
    "\n",
    "**Fifth**, it is good practice to work with as many risk factors as are necessary to capture 95\\% of the variance of $X^{dm}$. You could then apply any parametric or non-parametric time-series model to each risk factor to model its statistical characteristics. It is especially convenient to use a parametric model that scales linearly; such as ARMA-GARCH models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quizzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see ilias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: Challenging  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the (i) rational and (implementation) of one smart beta strategy of your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write the Fama-French 3-factor model as a multi-factor beta model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Describe a practical investment related problem where the solution is a Gram-Schmidt Orthogonalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explain the Gram-Schmidt Orthogonalization in terms of a least-squares linear regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Re-write a multi-factor linear pricing model with the Gram-Schmidt orthogonalized factors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write down expected return - risk relationship of Merton's Intertemporal CAPM. Explain an investment related setting where the relation makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Explain the research design of the Petkova (2006, JF) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Use a multi-factor linear pricing model to state return innovations as the sum of systematic and diversifiable innovations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Explain how to estimate a covariance matrix using (i) time-series data vs (ii) a linear factor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Assume you observe a return time-series for 50 different assets. Explain in detail how you estimate the unconditioanl factor exposure and unconditional factor premiums of the Fama-French 3-factor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Explain an investment strategy to collect a static risk premium of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Explain the behavioral biases that could explain why HMB is positive on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Explain how to implement and earn a dynamic risk premium of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Explain the concept of return anomalies and the state of the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Describes situations you have come across in your personal or professionell life where a PCA would have helped to overcome a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Assume you are given an unknown data panel. Explain in detail how a PCA analysis could help to understand the data and how you would perform such a PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Name and explain a couple of best practices when working with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
